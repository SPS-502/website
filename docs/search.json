[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SPS 502 | Data Science",
    "section": "",
    "text": "Instructor\n\n   Dr. Chris Birdsall\n   Environmental Research Building 1149\n   chrisbirdsall@boisestate.edu\n\n\n\nCourse details\n\n   Thursdays\n   August 25–December 15, 2022\n   6:00-8:45 p.m.\n   Bronco Gymnasium 218\n   Slack\n\n\n\nContacting me\nE-mail and the course Slack group are the best ways to get in contact with me. I will try to respond to all messages within 24 hours"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "labs/Lab01.html",
    "href": "labs/Lab01.html",
    "title": "SPS 502 | Data Science",
    "section": "",
    "text": "The main goal of this lab is to introduce you to R and RStudio, which we will be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions.\nAn additional goal is to introduce you to git and GitHub, which is the collaboration and version control system that we will be using throughout the course. Github is extremely helpful for tracking down problems with your code and for managing collaboration, which we’ll do in some labs later in the semester.\nAs the labs progress, you are encouraged to explore beyond what the labs dictate; a willingness to experiment will make you a much better programmer. Before we get to that stage, however, you need to build some basic fluency in R. Today we begin with the fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands."
  },
  {
    "objectID": "labs/Lab01.html#project-name",
    "href": "labs/Lab01.html#project-name",
    "title": "SPS 502 | Data Science",
    "section": "Project name:",
    "text": "Project name:\nCurrently your project is called Untitled Project. Update the name of your project to be “Lab 01 - Hello R”.\n\n\n\n\n\n\nThe top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for \"YAML Ain't Markup Language\". It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document."
  },
  {
    "objectID": "labs/Lab01.html#yaml",
    "href": "labs/Lab01.html#yaml",
    "title": "SPS 502 | Data Science",
    "section": "YAML:",
    "text": "YAML:\nOpen the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document."
  },
  {
    "objectID": "labs/Lab01.html#commiting-changes",
    "href": "labs/Lab01.html#commiting-changes",
    "title": "SPS 502 | Data Science",
    "section": "Commiting changes:",
    "text": "Commiting changes:\nThen Go to the Git pane in your RStudio.\nIf you have made changes to your Rmd file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state that includes your changes. If you’re happy with these changes, write “Update author name” in the Commit message box and hit Commit.\n\n\n\n\n\nYou don’t have to commit after every change, this would get quite cumbersome. You should consider committing states that are meaningful to you for inspection, comparison, or restoration. In the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions."
  },
  {
    "objectID": "labs/Lab01.html#pushing-changes",
    "href": "labs/Lab01.html#pushing-changes",
    "title": "SPS 502 | Data Science",
    "section": "Pushing changes:",
    "text": "Pushing changes:\nNow that you have made an update and committed this change, it’s time to push these changes to the web! Or more specifically, to your repo on GitHub. Why? So that others can see your changes. And by others, we mean the course teaching team (your repos in this course are private to you and us, only).\nIn order to push your changes to GitHub, click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. This might feel cumbersome. Bear with me… We will teach you how to save your password so you don’t have to enter it every time. But for this one assignment you’ll have to manually enter each time you push in order to gain some experience with it."
  },
  {
    "objectID": "labs/Lab01.html#thought-exercise",
    "href": "labs/Lab01.html#thought-exercise",
    "title": "SPS 502 | Data Science",
    "section": "Thought exercise:",
    "text": "Thought exercise:\nFor which of the above steps (changing project name, making updates to the document, committing, and pushing changes) do you need to have an internet connection? Discuss with your classmates."
  },
  {
    "objectID": "labs/Lab01.html#open-the-starter-project-in-rstudio-cloud",
    "href": "labs/Lab01.html#open-the-starter-project-in-rstudio-cloud",
    "title": "SPS 502 | Data Science",
    "section": "Open the starter project in RStudio Cloud",
    "text": "Open the starter project in RStudio Cloud\nVisit RStudio.cloud and login using the credentials you setup in our first class. Once you load up RStudio Cloud in your web browser you should see something like this:\n\nIn the left side bar you’ll see a header, “Space”, under which you’ll see “Your Workspace” and “SPS 502-F22”. Click SPS 502-F22 to view the list of projects associated with our class.\nThere you’ll see the Lab 01 starter project, called “Lab 01 - Hello R!”.\nClick “Start” to begin the lab"
  },
  {
    "objectID": "labs/Lab01.html#opening-the-starter-file",
    "href": "labs/Lab01.html#opening-the-starter-file",
    "title": "SPS 502 | Data Science",
    "section": "Opening the starter file",
    "text": "Opening the starter file\nWhen you want to write a paper, you have to open a Word document to type your ideas into, and save your work in. In R we use a document type called an R Markdown document. R Markdown documents are useful for both running code, and annotating the code with comments. The document can be saved, so you can refer back to your code later, and can be used to create other document types (html, word, pdf, or slides) for presenting the results of your analyses. R Markdown provides a way to generate clear and reproducible statistical analyses."
  },
  {
    "objectID": "labs/Lab01.html#editing-an-r-markdown-file",
    "href": "labs/Lab01.html#editing-an-r-markdown-file",
    "title": "SPS 502 | Data Science",
    "section": "Editing an R Markdown file",
    "text": "Editing an R Markdown file\nThe top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML it contains meta information about your document, such as the title of the document, the author name, and the date.\nLet’s start by adding your first and last name in author field and adding the date. Be sure to keep the quotation marks."
  },
  {
    "objectID": "labs/Lab01.html#saving-a-file",
    "href": "labs/Lab01.html#saving-a-file",
    "title": "SPS 502 | Data Science",
    "section": "Saving a file",
    "text": "Saving a file\nYou will complete your lab work in an R Markdown file like this each week, so it is important to learn how to save these files. It’s generally good practice to periodically save your work as you go.\nClick File > Save\nOkay, let’s get to the fun part!"
  },
  {
    "objectID": "labs/Lab01.html#loading-packages",
    "href": "labs/Lab01.html#loading-packages",
    "title": "SPS 502 | Data Science",
    "section": "Loading Packages",
    "text": "Loading Packages\nIn this lab we will use the tidyverse and datasauRus packages. We can load them using the following (this code is already provided for you in your starter document):\n\nlibrary(tidyverse)\n\nlibrary(datasauRus)"
  },
  {
    "objectID": "labs/Lab01.html#data",
    "href": "labs/Lab01.html#data",
    "title": "SPS 502 | Data Science",
    "section": "Data",
    "text": "Data\nThe data frame we will be working with today is called datasaurus_dozen and it’s in the datasauRus package. Actually, this single data frame contains 13 datasets, designed to show us why data visualization is important and how summary statistics alone can be misleading. The different datasets are marked by the dataset variable.\nTo find out more about the dataset, type the following in your Console: ?datasaurus_dozen. A question mark before the name of an object will always bring up its help file. This command must be run in the Console.\n\nBased on the help file, how many rows and how many columns does the datasaurus_dozen file have? What are the variables included in the data frame? Add your responses to your lab report.\n\nLet’s take a closer look at these datasets by making a frequency table of the dataset variable. Add the following code block to your R Markdown document and run it (copy the code then paste it under your answer in exercise 1):\n\ndatasaurus_dozen %>%\ncount(dataset) %>%\nprint(13)\n\nYou should see a “tibble” pop up listing the names of the datasets in one column and the number of observations (n) in the next column."
  },
  {
    "objectID": "labs/Lab01.html#data-visualization-and-summary",
    "href": "labs/Lab01.html#data-visualization-and-summary",
    "title": "SPS 502 | Data Science",
    "section": "Data Visualization and Summary",
    "text": "Data Visualization and Summary\nOkay, we’re about to do a lot of stuff that won’t make a ton of sense yet. We’ll go through a brief explanation in this lab, but more details and context for these commands will come in the next couple of weeks. For now, buckle up!\n\nPlot y vs. x for the dino dataset. Then, calculate the correlation coefficient between x and y for this dataset. (Er… excuse me?)\n\nDon’t worry. All the code you need to accomplish this is already in your lab for you. You’ll just need to include the code and some other information in your R Markdown file.\nStart with the datasaurus_dozen and pipe it into the filter function to filter for observations where dataset == \"dino\". Store the resulting filtered data frame as a new data frame called dino_data.\n\ndino_data <- datasaurus_dozen %>%\n\nfilter(dataset == \"dino\")\n\nThere is a lot going on here, so let’s go through each part:\nFirst, the pipe operator: %>%, takes what comes before it and sends it as the first argument to what comes after it. So here, we’re saying filter the datasaurus_dozen data frame for observations where dataset == \"dino\".\nSecond, the assignment operator: <-, assigns the name dino_data to the filtered data frame and stores it as an object in your environment (take a look at the environment pane in the top left corner).\nNext, we need to visualize these data (yes, plural). We will use the ggplot function for this. The first argument specifies the data you’re visualizing. Next we define the aesthetic mappings (more on this next week). In other words, the columns of the data that get mapped to certain aesthetic features of the plot, e.g. the x axis will represent the variable called x and the y axis will represent the variable called y. Then, we add another layer to this plot where we define which geometric shapes we want to use to represent each observation in the data. In this case we want these to be points, hence geom_point.\n\nggplot(data = dino_data, mapping = aes(x = x, y = y)) +\n\ngeom_point()\n\nIf this seems like a lot, it is. And you will learn about the philosophy of building data visualizations in detail next week. For now, just stick with it and follow along with the code . Again, I do not expect this to make much sense yet.\nFor the second part of this exercise, we need to calculate a summary statistic: the correlation coefficient. Correlation coefficients, often referred to as \\(r\\) in statistics, measure the linear association between two variables. You will see that some of the pairs of variables we plot do not have a linear relationship between them. This is exactly why we want to visualize first: visualize to assess the form of the relationship, and calculate \\(r\\) only if relevant. In this case, calculating a correlation coefficient really doesn’t make sense since the relationship between x and y is definitely not linear…. it’s shaped like a dinosaur!\nBut, for illustrative purposes, let’s calculate the correlation coefficient between x and y anyway.\n\n\nStart with dino_data and calculate a summary statistic that we will call r as the correlation between x and y.\n\ndino_data %>%\n  summarize(r = cor(x, y))\n\n\nPlot y vs. x for the star dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?\nPlot y vs. x for the circle dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?\nFinally, let’s plot all datasets at once. In order to do this we will make use of facetting.\n\n\nggplot(datasaurus_dozen, aes(x = x, y = y, color = dataset))+\n  geom_point()+\n  facet_wrap(~ dataset, ncol = 3) +\n  theme(legend.position = \"none\")\n\n\n\nFacet by the dataset variable, placing the plots in a 3 column grid, and don’t add a legend.\nAnd we can use the group_by function to generate all the summary correlation coefficients.\n\ndatasaurus_dozen %>%\n  group_by(dataset) %>%\n  summarize(r = cor(x, y)) %>%\n  print(13)"
  },
  {
    "objectID": "labs/Lab01.html#knitting-an-html-file",
    "href": "labs/Lab01.html#knitting-an-html-file",
    "title": "SPS 502 | Data Science",
    "section": "Knitting an HTML file",
    "text": "Knitting an HTML file\nClick the Knit button at the top left side of the screen to “knit” the file, or in other words, produce an output document. An .html file will be generated.\nNote: If you see the popup below you can just hit cancel:\n\nThen click on the html file in the “Files” pane to open it in your web browser:"
  },
  {
    "objectID": "labs/Lab01.html#finishing-touches",
    "href": "labs/Lab01.html#finishing-touches",
    "title": "SPS 502 | Data Science",
    "section": "Finishing touches",
    "text": "Finishing touches\nAlmost done, but I’d like you to do two more things:\n\nResize your figures:\n\nClick on the gear icon in on top of the R Markdown document, and select “Output Options…” in the dropdown menu. In the pop up dialogue box go to the Figures tab and change the height and width of the figures, and hit OK when done. Then, knit your document and see how you like the new sizes. Change and knit again and again until you’re happy with the figure sizes. Note that these values get saved in the YAML.\n\nYou can also use different figure sizes for different figures. To do so click on the gear icon within the chunk where you want to make a change. Changing the figure sizes added new options to these chunks: fig.width and fig.height. You can change them by defining different values directly in your R Markdown document as well.\n\n\nChange the look of your report:\n\nOnce again click on the gear icon in on top of the R Markdown document, and select “Output Options…” in the dropdown menu. In the General tab of the pop up dialogue box try out different Syntax highlighting and theme options. Hit OK and knit your document to see how it looks. Play around with these until you’re happy with the look."
  },
  {
    "objectID": "labs/Lab01.html#submission",
    "href": "labs/Lab01.html#submission",
    "title": "SPS 502 | Data Science",
    "section": "Submission",
    "text": "Submission\nFinally, you need to save your html file and submit it to me on Canvas. Here’s how to do it (see image below for reference):\n\n\nFirst, click the checkbox next to the html file in your files pane.\nNext, click “More” and select “Export”\nSave the html file somewhere on your computer where you can find it (e.g., downloads folder, documents folder, desktop, or a folder you created for the class).\nGo to our class page on canvas and click on “assignments”. Navigate to “Lab 01” under “Labs” and submit just as you would a paper in any other class.\n\nAnd you’re done!"
  },
  {
    "objectID": "labs/Lab02.html#packages",
    "href": "labs/Lab02.html#packages",
    "title": "SPS 502 | Data Science",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for this analysis. Run the following code to load this package.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "labs/Lab02.html#data",
    "href": "labs/Lab02.html#data",
    "title": "SPS 502 | Data Science",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a csv file in the data folder in the files pane. You can read it in using the following:\n\nplastic_waste <- read_csv(\"data/plastic-waste.csv\")\n\nThe variable descriptions are as follows:\n\ncode: 3 Letter country code\nentity: Country name\ncontinent: Continent name\nyear: Year\ngdp_per_cap: GDP per capita constant 2011 international $, rate\nplastic_waste_per_cap: Amount of plastic waste per capita in kg/day\nmismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day\nmismanaged_plastic_waste: Tonnes of mismanaged plastic waste\ncoastal_pop: Number of individuals living on/near coast\ntotal_pop: Total population according to Gapminder"
  },
  {
    "objectID": "labs/Lab02.html#submission",
    "href": "labs/Lab02.html#submission",
    "title": "SPS 502 | Data Science",
    "section": "Submission",
    "text": "Submission\nFinally, you need to save your html file and submit it to me on Canvas. Here’s how to do it (see image below for reference):\n\n\nFirst, click the checkbox next to the html file in your files pane.\nNext, click “More” and select “Export”\nSave the html file somewhere on your computer where you can find it (e.g., downloads folder, documents folder, desktop, or a folder you created for the class).\nGo to our class page on canvas and click on “assignments”. Navigate to “Lab 01” under “Labs” and submit just as you would a paper in any other class.\n\nAnd you’re done!"
  },
  {
    "objectID": "labs/Lab02.html#exercise-1",
    "href": "labs/Lab02.html#exercise-1",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 1",
    "text": "Exercise 1\nPlot, using histograms, the distribution of plastic waste per capita faceted by continent. What can you say about how the continents compare to each other in terms of their plastic waste per capita? Write and run the code required to create the faected histogram plot in your RMarkdown file.\n\n\nNOTE: Moving forward, the plots and the output of the code are not displayed in the lab instructions, but, as previously stated, you can and should copy the code to your R Markdown file and view the results yourself.\n\nDesnsity plots\nAnother way of visualizing numerical data is using density plots.\n\nggplot(data = plastic_waste, aes(x = plastic_waste_per_cap)) +\n  geom_density()\n\nAnd compare distributions across continents by coloring density curves by continent.\n\nggplot(data = plastic_waste, \n       mapping = aes(x = plastic_waste_per_cap, \n                     color = continent)) +\n  geom_density()\n\nThe resulting plot may be a little difficult to read, so let’s also fill the curves in with colors as well.\n\nggplot(data = plastic_waste, \n       mapping = aes(x = plastic_waste_per_cap, \n                     color = continent, \n                     fill = continent)) +\n  geom_density()\n\nThe overlapping colors make it difficult to tell what’s happening with the distributions in continents plotted first, and hence covered by continents plotted over them. We can change the transparency level of the fill color to help with this. The alpha argument takes values between 0 and 1 (e.g., 0.5): 0 is completely transparent and 1 is completely opaque. There is no way to tell what value will work best, so you just need to try a few.\n\nggplot(data = plastic_waste, \n       mapping = aes(x = plastic_waste_per_cap, \n                     color = continent, \n                     fill = continent)) +\n  geom_density(alpha = 0.7)\n\nThis still doesn’t look great…"
  },
  {
    "objectID": "labs/Lab02.html#exercise-2",
    "href": "labs/Lab02.html#exercise-2",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 2",
    "text": "Exercise 2\nRecreate the density plots above using a different (lower) alpha level that works better for displaying the density curves for all continents."
  },
  {
    "objectID": "labs/Lab02.html#exercise-3",
    "href": "labs/Lab02.html#exercise-3",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhy do we define the color and fill of the curves by mapping aesthetics of the plot, but define the alpha level as a characteristic of the plotting geom?\n\n\nHint: you may need to go back and read about the alpha argument in ModernDive to answer this question.\nAnd yet another way to visualize this relationship is using side-by-side box plots.\n\nggplot(data = plastic_waste, \n       mapping = aes(x = continent, \n                     y = plastic_waste_per_cap)) +\n  geom_boxplot()"
  },
  {
    "objectID": "labs/Lab02.html#exercise-4",
    "href": "labs/Lab02.html#exercise-4",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 4",
    "text": "Exercise 4\nConvert your side-by-side box plots from the previous task to violin plots. What do the violin plots reveal that box plots do not? What features are apparent in the box plots but not in the violin plots?"
  },
  {
    "objectID": "labs/Lab02.html#exercise-5",
    "href": "labs/Lab02.html#exercise-5",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 5",
    "text": "Exercise 5\nVisualize the relationship between plastic waste per capita and mismanaged plastic waste per capita using a scatterplot. Describe the relationship.\n\n\nRemember: We use geom_point() to make scatterplots."
  },
  {
    "objectID": "labs/Lab02.html#exercise-6",
    "href": "labs/Lab02.html#exercise-6",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 6",
    "text": "Exercise 6\nColor the points in the scatterplot by continent. Does there seem to be any clear distinctions between continents with respect to how plastic waste per capita and mismanaged plastic waste per capita are associated?"
  },
  {
    "objectID": "labs/Lab02.html#exercise-7",
    "href": "labs/Lab02.html#exercise-7",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 7",
    "text": "Exercise 7\nVisualize the relationship between plastic waste per capita and total population as well as plastic waste per capita and coastal population. You will need to make two separate plots. Do either of these pairs of variables appear to be more strongly linearly associated?"
  },
  {
    "objectID": "hw/hw01.html#logistics-and-tips-for-success",
    "href": "hw/hw01.html#logistics-and-tips-for-success",
    "title": "SPS 502 | Data Science",
    "section": "Logistics and tips for success",
    "text": "Logistics and tips for success\nThis lab will require you to write some of your own code (similar to lab 2). Read these instructions carefully as you work through the homework assignment.\n\nHow to answer questions\nSome questions will direct you to write code, while others will direct you to write a response. You’ll see TYPE YOUR ANSWER HERE in your RMarkdown file in places where I expect you to write a response. And you’ll see code blocks with #Delete and enter your code where I expect you to write your own code.\n\n\nCopying/pasting and writing code\nCopying and editing code from other answers or examples is a great way to both save time and learn. If you copy code over and something isn’t working, look carefully at the edits you made to see whether you missed something in the copying/pasting process. Also, be sure you’ve edited ALL the variable/object names appropriately so you’re accomplishing what the question is asking you to do. You should also make sure that you’re typing the variable and object names EXACTLY as they appear in the dataset or environment pane (spelling and case must match exactly).\n\n\nIf you’re stuck\nIf you get stuck, don’t panic! Before you ask for help, give yourself a few minutes to figure it out. Search ModernDive, look at your previous labs, and google your problem with “tidyverse”. Also, step away from your computer for a few minutes if you need a break. Finally (and this doesn’t have to be the last step), ask for help on Slack!"
  },
  {
    "objectID": "hw/hw01.html#load-packages",
    "href": "hw/hw01.html#load-packages",
    "title": "SPS 502 | Data Science",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)"
  },
  {
    "objectID": "hw/hw01.html#import-data",
    "href": "hw/hw01.html#import-data",
    "title": "SPS 502 | Data Science",
    "section": "Import Data",
    "text": "Import Data\nNext, let’s load the data:\n\n# Load raw data\nidaho_crashes_raw <- read_csv(\"data/Crash_Data_2005__Present.csv\")\n\nRows: 299135 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Severity, County, IntersectionRelated\ndbl  (5): OBJECTID, Mile_Point, Accident_Year, Number_Of_Fatalities, Number_...\ndttm (1): Accident_Date_Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWhat do the data look like? Here are a few ways to view data in r\n\nidaho_crashes_raw\n\n\nglimpse(idaho_crashes_raw)\n\n\nstr(idaho_crashes_raw)\n\nRun the already provided code in your Rmarkdown file to get a sense of the dataset. More detail about these commands is available in ModernDive."
  },
  {
    "objectID": "hw/hw01.html#data-overview",
    "href": "hw/hw01.html#data-overview",
    "title": "SPS 502 | Data Science",
    "section": "Data Overview",
    "text": "Data Overview\n\nQuestion 1\n\nWhat is another method you’ve used/learned to View a dataset? (HINT: The command is in the question). Type the code in the code block under “Question 1” in your RMarkdown file in RStudio Cloud. If you have no idea go back to ModernDive Chapter 1 (sec 1.4.3).\n\n\n\nQuestion 2\n\nWhat have we learned about the data? How many car accidents (observations/rows) do we have? How many variables (columns)? Type your answers in your RMarkdown sheet."
  },
  {
    "objectID": "hw/hw01.html#clean-the-data",
    "href": "hw/hw01.html#clean-the-data",
    "title": "SPS 502 | Data Science",
    "section": "Clean the data",
    "text": "Clean the data\nOkay, let’s clean up the raw data a little and get some useful information along the way. First, we’ll use the lubridate package to get useful information from the Accident_Date_Time variable, as well as rename a variable to make it easier to remember (we’ll cover more about the lubridate package in the future).\n\nidaho_crashes_clean <- idaho_crashes_raw %>% \n  mutate(Hour= hour(Accident_Date_Time),\n         Month = month(Accident_Date_Time, label = TRUE, abbr = TRUE),\n         Day = wday(Accident_Date_Time, label = TRUE, abbr = TRUE)) %>%\n    rename(Year = Accident_Year)\n\nThe lubridate package allowed us to extract useful information from the Accident_Date_Time variable.\nRun the code below to learn more about the lubridate package:\n\n?lubridate\n\nNow we have a “cleaned” version of the data stored in a dataframe called ‘idaho_crashes_clean’.\nLet’s take another look at the data:\n\nidaho_crashes_clean\n\n\nQuestion 3\n\nWhat’s new in this dataset (idaho_crashes_clean) compared to the raw version? Write your answer in your RMarkdown sheet."
  },
  {
    "objectID": "hw/hw01.html#accidents-over-time",
    "href": "hw/hw01.html#accidents-over-time",
    "title": "SPS 502 | Data Science",
    "section": "Accidents over time",
    "text": "Accidents over time\nNow that we have our data in shape, let’s see if we can gain any insights about car accidents in Idaho.\n\nQuestion 4\n4a. First, has the number of accidents increased since 2005?\nThings to try:\nCreate a new dataframe that groups the number of accidents per year:\n\naccidents_per_year <- idaho_crashes_clean %>% \n  count(Year)\n\n#Print the result \naccidents_per_year\n\n# A tibble: 13 × 2\n    Year     n\n   <dbl> <int>\n 1  2005 28137\n 2  2006 24156\n 3  2007 26452\n 4  2008 25069\n 5  2009 22625\n 6  2010 22170\n 7  2011 20600\n 8  2012 11459\n 9  2013 21387\n10  2014 22279\n11  2015 23904\n12  2016 25181\n13  2017 25716\n\n\nTry adding , sort = TRUE in count() in the code above and run the command again in another code block. Copy the code and paste it into the code block provided in your RMarkdown file. Then edit the code accordingly and run it again.\n4b. Why would you want to sort this list? Why would you not want to sort this list? Write your answer in your RMarkdown file.\n\n\nQuestion 5\nNext, let’s try a visualization:\n\nggplot(data = accidents_per_year, \n       mapping = aes(x = Year, y = n)) +\n      labs(y = \"Car Crashes\",\n           x = \"Year\") +\n  scale_x_continuous(breaks = pretty_breaks())+\n  geom_line()\n\n\n\n\n\nNotice anything strange? What’s going on in 2012? Which did you find more informative, the tibble (table with the count of accidents) or the visualization? Write your answer in your RMarkdown file."
  },
  {
    "objectID": "hw/hw01.html#accidents-by-month",
    "href": "hw/hw01.html#accidents-by-month",
    "title": "SPS 502 | Data Science",
    "section": "Accidents by month",
    "text": "Accidents by month\n\nQuestion 6\n\nWhich is the most dangerous month? Let’s create a new dataframe that groups accidents by month then create a line graph to visualize the data. Try writing the code yourself this time. (HINT: the code is nearly identical to the three codeblocks above)."
  },
  {
    "objectID": "hw/hw01.html#accidents-by-time-of-day",
    "href": "hw/hw01.html#accidents-by-time-of-day",
    "title": "SPS 502 | Data Science",
    "section": "Accidents by time of day",
    "text": "Accidents by time of day\nLet’s see what the worst time of day is for accidents.\nWe have the hour of the crash recorded in the variable Hour, which lists the hour from 0 to 23.\n\nidaho_crashes_clean %>% \n  count(Hour, sort = TRUE)\n\n# A tibble: 24 × 2\n    Hour     n\n   <int> <int>\n 1    17 25822\n 2    16 24816\n 3    15 24368\n 4    12 19454\n 5    14 19305\n 6    18 18277\n 7    13 18230\n 8    11 15617\n 9     7 15519\n10     8 14967\n# … with 14 more rows\n\n\n\nQuestion 7\n\nWhich are the top 3 worst hours for car accidents? What time of day is this? Type your answer in the RMarkdown file."
  },
  {
    "objectID": "hw/hw01.html#accidents-by-county",
    "href": "hw/hw01.html#accidents-by-county",
    "title": "SPS 502 | Data Science",
    "section": "Accidents by county",
    "text": "Accidents by county\n\nQuestion 8\n8a. Which is the most dangerous county? Run the code below to find out!\n\naccidents_by_county <- idaho_crashes_clean %>%\n  count(County, sort = TRUE)\n\n#Print the result\naccidents_by_county\n\n8b. Are you surprised? Why or why not?\n8c. This may not be the best measure to determine which is the most dangerous county. Why? What might be a better measure? Do we need additional data?"
  },
  {
    "objectID": "hw/hw01.html#fatal-accidents",
    "href": "hw/hw01.html#fatal-accidents",
    "title": "SPS 502 | Data Science",
    "section": "Fatal Accidents",
    "text": "Fatal Accidents\nThe dataset has information on whether accidents are fatal recorded in the variable, Number_Of_Fatalities. Let’s focus on those and see if the trends are any different.\nThere are a number of ways to do this, but let’s keep it simple for now and use the subset function to create a new dataframe which only includes fatal accidents (i.e. observations where number of fatalities is greater than zero).\n\nidaho_fatal_crashes <- subset(idaho_crashes_clean, subset = Number_Of_Fatalities > 0)\n\n#Print the result\nidaho_fatal_crashes\n\nAlright. A much smaller dataset. Let’s dig in a bit.\n\nQuestion 9\nFirst, let’s see if the most dangerous counties are also the deadliest. You’ll need to write this code yourself.\n9a. Create a new dataframe that counts the number of fatal crashes per county and print the result.\n9b. Next, create a visualization showing fatal crashes by county. Enter your code in the codeblocks provided in the RMarkdown file.\n9c. What are the similarities and differences between this and the original county list? Type your answer in the RMarkdown file.\n\n\nHint: The code will be very, very similar to the code in Question 8 (in codeblock count-county) and the code in Question 5 (in codeblock count-year-viz). You will just need to figure out which inputs to replace so that you’re using the right data.\n\n\nQuestion 10\nNow let’s look at fatal accidents by month.\n10a. Create a new dataframe counting fatal accidents by month.\n10b. Next, create a linegraph to visualize the data.\nBased on your tables and visualizations, how do fatal accidents by month differ from total accidents by month? What might explain the differences? Provide your code and your written answer in your RMarkdown file."
  },
  {
    "objectID": "hw/hw01.html",
    "href": "hw/hw01.html",
    "title": "SPS 502 | Data Science",
    "section": "",
    "text": "Due by 11:59 p.m., Friday September 16"
  },
  {
    "objectID": "hw/hw01.html#exploring-the-data",
    "href": "hw/hw01.html#exploring-the-data",
    "title": "SPS 502 | Data Science",
    "section": "Exploring the Data",
    "text": "Exploring the Data\n\nQuestion 1\n\nWhat is another method you’ve used/learned to View a dataset? (HINT: The command is in the question). Type the code in the code block under “Question 1” in your RMarkdown file in RStudio Cloud.\n\n\n\nQuestion 2\n\nWhat have we learned about the data? How many car accidents (observations/rows) do we have? How many variables (columns)? Type your answers in your RMarkdown sheet."
  },
  {
    "objectID": "labs/Lab03.html#packages",
    "href": "labs/Lab03.html#packages",
    "title": "SPS 502 | Data Science",
    "section": "Packages",
    "text": "Packages\nRun the following to load the necessary packages for this lab:\n\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "labs/Lab03.html#data",
    "href": "labs/Lab03.html#data",
    "title": "SPS 502 | Data Science",
    "section": "Data",
    "text": "Data\nRun the following to load and take a glimpse of the data:\n\ndata(txhousing)\nglimpse(txhousing)\n\nThese data are about housing in Texas. Each row is monthly data for a given city in Texas in a given year. There are multiple years of data for each city."
  },
  {
    "objectID": "labs/Lab03.html#exercise-1",
    "href": "labs/Lab03.html#exercise-1",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 1",
    "text": "Exercise 1\nTake a look at the data in the data viewer. You can accomplish this two different ways: A) click on the name of the data in the Environment pane, or b) type View(txhousing) in the console. What is the last city listed in the data set (in row 8602)?\n\n\nIn homework 1 we learned (and I was reminded) that RStudio does not like to knit documents that execute the View() command. So from now on, we’ll just run it in the console to avoid additional headaches."
  },
  {
    "objectID": "labs/Lab03.html#exercise-2",
    "href": "labs/Lab03.html#exercise-2",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 2",
    "text": "Exercise 2\nTake a look at the variable descriptions by typing ?txhousing into the console. What is the listings variable in this data set?"
  },
  {
    "objectID": "labs/Lab03.html#select",
    "href": "labs/Lab03.html#select",
    "title": "SPS 502 | Data Science",
    "section": "select",
    "text": "select\nSometimes we want to pull out or extract just one or two columns of data. The following code will extract only the column in the data set for the variables sales and volume.\n\ntxhousing %>% select(sales, volume)\n\nThe %>% symbol is called the piping operator. Here, it takes the txhousing data frame and “pipes” or feeds it into the select function. You can think of the %>% symbol as the word “then”.\nNote that we did not use an assignment operator <- so we did not save these extracted, selected values. In the following code, we save the results, in a data frame ASLO called txhousing. By putting - in front of the date variable we tell R to select all except the date variable. Run the following code:\n\ntxhousing <- txhousing %>% select(-date)\n\nIf you look at txhousing in the data viewer, the date variable is no longer included."
  },
  {
    "objectID": "labs/Lab03.html#filter",
    "href": "labs/Lab03.html#filter",
    "title": "SPS 502 | Data Science",
    "section": "filter",
    "text": "filter\nThe filter function allows you to pull out just the rows (cases or observations) you want, based on some criteria in one of the columns.\nImagine for instance that we wanted to reduce the data set include data for only 2012, in Austin. This code chunk takes the txhousing data, then filters it to only include rows in which the year is 2012, and the city is Austin. The results are saved in a new data frame called austin_12 that shows up in the workspace.\n\naustin_12 <- txhousing %>% filter(year == 2012, city == \"Austin\")\n\n\nNote that we use == to identify the desired criteria.\n\nWhat if we wanted to restrict our data set to only years before 2004 and the City of Austin? Below we use the < symbol to accomplish this. Note we did not SAVE these results in a new data frame…so no new data frame showed up in our Environment pane, but the results print out immediately below the code chunk.\n\ntxhousing %>% filter(year < 2004, city == \"Austin\")\n\nWhat if we wanted to use multiple cities? Below we use the | symbol to indicate that the city could be Austin OR Abilene. In this case, we saved these results as a new data frame called aust_ab that appears in your Environment pane.\n\naust_ab <- txhousing %>% filter(city == \"Austin\" | city == \"Abilene\")"
  },
  {
    "objectID": "labs/Lab03.html#mutate",
    "href": "labs/Lab03.html#mutate",
    "title": "SPS 502 | Data Science",
    "section": "mutate",
    "text": "mutate\nThe mutate function can add new columns (variables) to a data frame. For instance, the following will add a new column to the data called vol_100k that expresses volume in units of $100000.\n\ntxhousing <- txhousing %>%\n  mutate(vol_100k = volume/100000)\n\nNote that we SAVED these results in new data frame called txhousing. This therefore overwrote the old txhousing data frame with a new version that contains this column. You can open the txhousing data frame in the viewer to confirm that it now contains this new column."
  },
  {
    "objectID": "labs/Lab03.html#summarize",
    "href": "labs/Lab03.html#summarize",
    "title": "SPS 502 | Data Science",
    "section": "summarize",
    "text": "summarize\nOne of the first tasks in data analysis is often to get descriptive statistics that help to understand the central tendency and variability in the data. The summarize() command can take a column of data, and reduce it to a summary statistic.\nFor instance, the code below uses the austin_12 data set made earlier to calculate the mean monthly number of sales in Austin in 2012.\n\naustin_12 %>% summarize(x_bar_sales = mean(sales))\n\nThis code tells R to calculate the mean of the variable sales, and to save the results in a variable called x_bar_sales.\nYou can also calculate multiple summary statistics at once, and even for multiple variables. Below we also calculate a standard deviation sd() of sales, a minimum min() of the volume variable, a maximum max() of the volume variable, etc. The n() calculates sample size…or the number of rows/ cases in the data frame.\n\naustin_12 %>% summarize(x_bar_sales = mean(sales), \n                        sd_sales = sd(sales), \n                        min_vol = min(volume), \n                        max_vol = max(volume), \n                        mdn_list = median(listings), \n                        iqr_list = IQR(listings),\n                        sample_size = n())\n\n# A tibble: 1 × 7\n  x_bar_sales sd_sales   min_vol   max_vol mdn_list iqr_list sample_size\n        <dbl>    <dbl>     <dbl>     <dbl>    <dbl>    <dbl>       <int>\n1       2127.     501. 265821275 791281075     7925     949.          12\n\n\nNote that the names of the elements you calculate are user defined, like xbar_sales, min_vol, and mdn_list. You could customize these names as you like (but don’t use spaces in your names)."
  },
  {
    "objectID": "labs/Lab03.html#arrange",
    "href": "labs/Lab03.html#arrange",
    "title": "SPS 502 | Data Science",
    "section": "arrange",
    "text": "arrange\nYou just determined that the maximum volume of monthly sales in Austin in 2012 was a total of $791,281,075 ….but what if you wanted to know WHAT MONTH that occurred in? Copy paste, and run the following into a new code chunk:\n\naustin_12 %>%\n  arrange(desc(volume))\n\nThis tells R to arrange the rows in the data set based on the volume column, and to do so in descending order. So the row with the $791,281,075 in sales is shown at the top! We can see that this volume occurred in the sixth month (June)."
  },
  {
    "objectID": "labs/Lab03.html#group_by",
    "href": "labs/Lab03.html#group_by",
    "title": "SPS 502 | Data Science",
    "section": "group_by",
    "text": "group_by\nSometimes we also want to calculate summary statistics across different levels of another variable. For instance, here we find the average number of monthly sales that occurred in Abilene and Austin across all years in the data set. Note that we use the aust_ab data frame we created earlier, to restrict our analysis to those two cities.\n\naust_ab %>% group_by(city) %>% \n  summarize(x_bar_sales = mean(sales))\n\n# A tibble: 2 × 2\n  city    x_bar_sales\n  <chr>         <dbl>\n1 Abilene        150.\n2 Austin        1997.\n\n\nFrom the results we can see that there were an average of 150 sales per month in Abilene, and 1996 in Austin.\nWe can give R multiple variables to group by. For instance, this code gives us the mean sales for each month in each city, averaged across all the years. So for instance the mean number of sales in January, in Abilene was 96 homes.\n\naust_ab %>% group_by(city, month) %>% \n  summarize(x_bar_sales = mean(sales))"
  },
  {
    "objectID": "labs/Lab03.html#exercise-3",
    "href": "labs/Lab03.html#exercise-3",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 3",
    "text": "Exercise 3\nWrite a code chunk to remove the inventory variable. Save the results in a data frame called txhousing. Confirm in the data viewer that the variable has been removed."
  },
  {
    "objectID": "labs/Lab03.html#exercise-4",
    "href": "labs/Lab03.html#exercise-4",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 4",
    "text": "Exercise 4\nMake a data set called dallas_sub that includes data only from the city of Dallas in 2012 & 2013."
  },
  {
    "objectID": "labs/Lab03.html#exercise-5",
    "href": "labs/Lab03.html#exercise-5",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 5",
    "text": "Exercise 5\nAdd a column to the dallas_sub data set called prct_sold that calculates the percentage of listings that were sold (sales/listings * 100). Be sure to save the results also as a data frame called dallas_sub."
  },
  {
    "objectID": "labs/Lab03.html#exercise-6",
    "href": "labs/Lab03.html#exercise-6",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 6",
    "text": "Exercise 6\nCalculate the average percentage of listings that were sold in Dallas in each month of the year based on your dallas_sub data set. Save the results of the calculation in an data frame called dallas_summary."
  },
  {
    "objectID": "labs/Lab03.html#exercise-7",
    "href": "labs/Lab03.html#exercise-7",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 7",
    "text": "Exercise 7\nArrange the dallas_summary in descending order based on the average percentage of listings, so you can see which month had the greatest average percentage of listings sold. You do not need to save the results."
  },
  {
    "objectID": "labs/Lab03.html#advanced-wrangling-exercises",
    "href": "labs/Lab03.html#advanced-wrangling-exercises",
    "title": "SPS 502 | Data Science",
    "section": "Advanced Wrangling Exercises",
    "text": "Advanced Wrangling Exercises\nYou may have to use multiple dplyr functions to answer each question. Think through the steps of how to get to the answer you are trying to find."
  },
  {
    "objectID": "labs/Lab03.html#exercise-8",
    "href": "labs/Lab03.html#exercise-8",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 8",
    "text": "Exercise 8\nRun the following code chunk. Study the code, and the output. Explain in your own words what this code chunk calculated.\n\ntxhousing %>% \n  filter(year == 2012 | year == 2013, city == \"Dallas\") %>%\n  mutate(prct_sold = sales/listings *100) %>%\n  group_by(month) %>%\n  summarize(mean_prct_sold = mean(prct_sold)) %>% \n  arrange(desc(mean_prct_sold))"
  },
  {
    "objectID": "labs/Lab03.html#exercise-9",
    "href": "labs/Lab03.html#exercise-9",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 9",
    "text": "Exercise 9\nIn January of 2015, what city had the fewest houses listed for sale?"
  },
  {
    "objectID": "labs/Lab03.html#exercise-10",
    "href": "labs/Lab03.html#exercise-10",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 10",
    "text": "Exercise 10\nIn 2012, in which month were the most houses sold in Texas?"
  },
  {
    "objectID": "labs/Lab03.html#exercise-11",
    "href": "labs/Lab03.html#exercise-11",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 11",
    "text": "Exercise 11\nGenerate a single table that shows the total number of houses sold in Austin in 2000 and 2001 (total over the entire period), and the total number of houses sold in Dallas in 2000 and 2001 (total over the entire period). This calculation requires a number of steps, so it might help you to first write out on paper the different steps you will need to take. That will help you set out a “blueprint” for tackling the problem.\n\n\nHint: recall the sum() function can add(+) values."
  },
  {
    "objectID": "labs/Lab03.html",
    "href": "labs/Lab03.html",
    "title": "SPS 502 | Data Science",
    "section": "",
    "text": "#Lab 03 - Data Wrangling"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#testing-this",
    "href": "lectures/lecture03/lecture03.html#testing-this",
    "title": "Data Wrangling",
    "section": "Testing this",
    "text": "Testing this\nTesting"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#mean",
    "href": "lectures/lecture03/lecture03.html#mean",
    "title": "Data Wrangling",
    "section": "Mean",
    "text": "Mean\n\n\nArithmetic Mean (\\(\\bar{x}\\) or \\(\\mu\\)): the sum of a series of observations divided by the number of observations \\[\\bar{x} = \\frac{1}{n}\\left (\\sum_{i=1}^n{x_i}\\right ) = \\frac{x_1+x_2+\\cdots +x_n}{n}\\]\nUse when describing continuous variables\nSensitive to extreme values (e.g., income)"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#median",
    "href": "lectures/lecture03/lecture03.html#median",
    "title": "Data Wrangling",
    "section": "Median",
    "text": "Median\n\n\nThe middle value in a series of observations\n\nArrange a series of observations from smallest to greatest then take the middle value\n\nNote: if there is an even number of observations, then there is no single middle value; the median is then usually defined to be the mean of the two middle values\n\n\nIn a distribution, the median is the \\(2^{nd}\\) quartile and the \\(50^{th}\\) percentile\nUse when describing continuous variables\nRobust to extreme values (e.g., income)"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#normal-distribution",
    "href": "lectures/lecture03/lecture03.html#normal-distribution",
    "title": "Data Wrangling",
    "section": "Normal Distribution",
    "text": "Normal Distribution"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#central-tendency-normal-distribution",
    "href": "lectures/lecture03/lecture03.html#central-tendency-normal-distribution",
    "title": "Data Wrangling",
    "section": "Central Tendency (Normal Distribution)",
    "text": "Central Tendency (Normal Distribution)"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#skewed-distribution",
    "href": "lectures/lecture03/lecture03.html#skewed-distribution",
    "title": "Data Wrangling",
    "section": "Skewed Distribution",
    "text": "Skewed Distribution"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#postive-skewed-distribution",
    "href": "lectures/lecture03/lecture03.html#postive-skewed-distribution",
    "title": "Data Wrangling",
    "section": "Postive Skewed Distribution",
    "text": "Postive Skewed Distribution"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#negative-skewed-distribution",
    "href": "lectures/lecture03/lecture03.html#negative-skewed-distribution",
    "title": "Data Wrangling",
    "section": "Negative Skewed Distribution",
    "text": "Negative Skewed Distribution"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#mode",
    "href": "lectures/lecture03/lecture03.html#mode",
    "title": "Data Wrangling",
    "section": "Mode",
    "text": "Mode\n\nMode: the most common value in a series of observations\n\nSome variables include multiple modes (multimodal)\n\nUse when describing discrete (e.g., categorical) variables"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#central-tendency-review",
    "href": "lectures/lecture03/lecture03.html#central-tendency-review",
    "title": "Data Wrangling",
    "section": "Central Tendency Review",
    "text": "Central Tendency Review\n\nArithmetic mean: sum of values divided by number of values\nMedian: middle value\nMode: most frequent value\nWhat is the mean, median, and mode of \\(x\\)?\n\n\\(x\\) = {1, 2, 2, 3, 4, 7, 9, 10, 11, 12, 12}"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#variance",
    "href": "lectures/lecture03/lecture03.html#variance",
    "title": "Data Wrangling",
    "section": "Variance",
    "text": "Variance\n\nVariance (\\(s^{2}\\), \\(\\sigma^{2}\\)): the extent to which a variable’s values deviate from or vary around it’s mean; the average distance of each observation from the mean \\[s^{2} = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\]"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#variance-1",
    "href": "lectures/lecture03/lecture03.html#variance-1",
    "title": "Data Wrangling",
    "section": "Variance",
    "text": "Variance\n\nMeasures how far observations are from their average value (spread of the data)\n\nLow variance \\(\\rightarrow\\) observations are relatively close to the mean value\nHigh variance \\(\\rightarrow\\) observations are relatively far from the mean value\n\nExpressed in square units (i.e., \\(\\text{inches}^2\\))"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#dispersion-standard-deviation",
    "href": "lectures/lecture03/lecture03.html#dispersion-standard-deviation",
    "title": "Data Wrangling",
    "section": "Dispersion (Standard Deviation)",
    "text": "Dispersion (Standard Deviation)\n\nStandard Deviation (\\(s\\), \\(\\sigma\\)): standardized measure of variance \\[s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}\\]\nExpressed in the same units as the mean (i.e., inches)\n\nMean distance from the mean"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#standard-deviation",
    "href": "lectures/lecture03/lecture03.html#standard-deviation",
    "title": "Data Wrangling",
    "section": "Standard Deviation",
    "text": "Standard Deviation\n\nStandard Deviation (\\(s\\), \\(\\sigma\\)): standardized measure of variance \\[s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}\\]\nExpressed in the same units as the mean (i.e., inches)\n\nMean distance from the mean"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#pipe",
    "href": "lectures/lecture03/lecture03.html#pipe",
    "title": "Data Wrangling",
    "section": "pipe %>%",
    "text": "pipe %>%\n\nleave_house(get_dressed(get_out_of_bed(wake_up(chris))))\n\n\n“Pipe” a data frame into a “verb” command\n“Chain” the results from one “verb” command into another\nThink of it as the word “then”\n\n\nWriting it out using pipes give it a more natural (and easier to read) structure\n\n\nchris %>% \n  wake_up() %>% \n  get_out_of_bed() %>% \n  get_dressed() %>% \n  leave_house()"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#pipe-1",
    "href": "lectures/lecture03/lecture03.html#pipe-1",
    "title": "Data Wrangling",
    "section": "pipe %>%",
    "text": "pipe %>%\n\nleave_house(get_dressed(get_out_of_bed(wake_up(chris))))\n\n\n“Pipe” a data frame into a “verb” command\n“Chain” the results from one “verb” command into another\nThink of it as the word “then”\n\n\nchris %>% \n  wake_up() %>% \n  get_out_of_bed() %>% \n  get_dressed() %>% \n  leave_house()"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#data-wrangling-1",
    "href": "lectures/lecture03/lecture03.html#data-wrangling-1",
    "title": "Data Wrangling",
    "section": "Data Wrangling",
    "text": "Data Wrangling"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#select",
    "href": "lectures/lecture03/lecture03.html#select",
    "title": "Data Wrangling",
    "section": "select()",
    "text": "select()"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#select-1",
    "href": "lectures/lecture03/lecture03.html#select-1",
    "title": "Data Wrangling",
    "section": "select()",
    "text": "select()"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#filter",
    "href": "lectures/lecture03/lecture03.html#filter",
    "title": "Data Wrangling",
    "section": "filter()",
    "text": "filter()"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#filter-1",
    "href": "lectures/lecture03/lecture03.html#filter-1",
    "title": "Data Wrangling",
    "section": "filter()",
    "text": "filter()"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#filter-2",
    "href": "lectures/lecture03/lecture03.html#filter-2",
    "title": "Data Wrangling",
    "section": "filter()",
    "text": "filter()"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#summarize",
    "href": "lectures/lecture03/lecture03.html#summarize",
    "title": "Data Wrangling",
    "section": "summarize()",
    "text": "summarize()\n\n\nSummary functions take many values and return one value meant to describe all those values as a group."
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#summarize-1",
    "href": "lectures/lecture03/lecture03.html#summarize-1",
    "title": "Data Wrangling",
    "section": "summarize()",
    "text": "summarize()\n\nidaho_crashes %>% \n  summarize(avg_injuries = mean(Number_Of_Injuries))\n\n# A tibble: 1 × 1\n  avg_injuries\n         <dbl>\n1        0.521"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#group_by-summarize",
    "href": "lectures/lecture03/lecture03.html#group_by-summarize",
    "title": "Data Wrangling",
    "section": "group_by() & summarize()",
    "text": "group_by() & summarize()\n\nidaho_crashes %>% \n  group_by(IntersectionRelated) %>% \n  summarize(avg_injuries = mean(Number_Of_Injuries))\n\n# A tibble: 2 × 2\n  IntersectionRelated avg_injuries\n  <chr>                      <dbl>\n1 N                          0.478\n2 Y                          0.583"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#summarize-2",
    "href": "lectures/lecture03/lecture03.html#summarize-2",
    "title": "Data Wrangling",
    "section": "summarize()",
    "text": "summarize()"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#mutate",
    "href": "lectures/lecture03/lecture03.html#mutate",
    "title": "Data Wrangling",
    "section": "mutate()",
    "text": "mutate()"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#mutate-1",
    "href": "lectures/lecture03/lecture03.html#mutate-1",
    "title": "Data Wrangling",
    "section": "mutate()",
    "text": "mutate()"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#traditional-nested-functions",
    "href": "lectures/lecture03/lecture03.html#traditional-nested-functions",
    "title": "Data Wrangling",
    "section": "Traditional Nested Functions",
    "text": "Traditional Nested Functions\n\nleave_house(get_dressed(get_out_of_bed(wake_up(chris))))"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#filter-to-select-a-subset-of-rows",
    "href": "lectures/lecture03/lecture03.html#filter-to-select-a-subset-of-rows",
    "title": "Data Wrangling",
    "section": "filter() to select a subset of rows",
    "text": "filter() to select a subset of rows\nfor crashes in Canyon county\n\nidaho_crashes %>%\n  filter(County == \"Canyon\") \n\n# A tibble: 35,492 × 12\n   OBJECTID Severity            Mile_Point  Year County Number…¹ Numbe…² Inter…³\n      <dbl> <chr>                    <dbl> <dbl> <chr>     <dbl>   <dbl> <chr>  \n 1    20294 Property Dmg Report      0.004  2005 Canyon        0       0 N      \n 2    21886 Property Dmg Report      5.25   2005 Canyon        0       0 Y      \n 3    26693 Property Dmg Report     NA      2005 Canyon        0       0 Y      \n 4    18673 Property Dmg Report      9.27   2005 Canyon        0       0 N      \n 5    17678 Property Dmg Report     NA      2005 Canyon        0       0 Y      \n 6    17287 B Injury Accident       NA      2005 Canyon        0       1 N      \n 7    26710 Property Dmg Report     NA      2005 Canyon        0       0 N      \n 8    19529 Property Dmg Report     39      2005 Canyon        0       0 N      \n 9    20899 B Injury Accident       50.0    2005 Canyon        0       1 Y      \n10    19655 B Injury Accident       23.5    2005 Canyon        0       1 N      \n# … with 35,482 more rows, 4 more variables: Accident_Date_Time <dttm>,\n#   Hour <int>, Month <ord>, Day <ord>, and abbreviated variable names\n#   ¹​Number_Of_Fatalities, ²​Number_Of_Injuries, ³​IntersectionRelated\n\n\n\ndbl = numerical chr = text data See moderndive 1.4.3 for more info"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#filter-for-many-conditions-at-once",
    "href": "lectures/lecture03/lecture03.html#filter-for-many-conditions-at-once",
    "title": "Data Wrangling",
    "section": "filter() for many conditions at once",
    "text": "filter() for many conditions at once\nfor crashes in Canyon county in 2017\n\nidaho_crashes %>%\n  filter(County == \"Canyon\", Year == 2017) \n\n# A tibble: 3,191 × 12\n   OBJECTID Severity            Mile_Point  Year County Number…¹ Numbe…² Inter…³\n      <dbl> <chr>                    <dbl> <dbl> <chr>     <dbl>   <dbl> <chr>  \n 1   295441 B Injury Accident         4.32  2017 Canyon        0       1 N      \n 2   288513 Fatal Accident           31.1   2017 Canyon        1       1 N      \n 3   293711 Property Dmg Report      31.1   2017 Canyon        0       0 N      \n 4   291456 Property Dmg Report     105.    2017 Canyon        0       0 Y      \n 5   306112 C Injury Accident        31.5   2017 Canyon        0       1 N      \n 6   299918 A Injury Accident        11.2   2017 Canyon        0       1 N      \n 7   295624 C Injury Accident        31.2   2017 Canyon        0       1 N      \n 8   304753 Property Dmg Report      59.3   2017 Canyon        0       0 Y      \n 9   293682 Property Dmg Report       3.03  2017 Canyon        0       0 Y      \n10   292901 Property Dmg Report      28.8   2017 Canyon        0       0 N      \n# … with 3,181 more rows, 4 more variables: Accident_Date_Time <dttm>,\n#   Hour <int>, Month <ord>, Day <ord>, and abbreviated variable names\n#   ¹​Number_Of_Fatalities, ²​Number_Of_Injuries, ³​IntersectionRelated"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#logical-operators-in-r",
    "href": "lectures/lecture03/lecture03.html#logical-operators-in-r",
    "title": "Data Wrangling",
    "section": "Logical operators in R",
    "text": "Logical operators in R\n\n\n\noperator\ndefinition\noperator\ndefinition\n\n\n\n\n<\n\n\n\n\n\n<=\n\n\n\n\n\n>\n\n\n\n\n\n>=\n\n\n\n\n\n==\n\n\n\n\n\n!=\n\n\n\n\n\nx & y"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#select-to-keep-variables",
    "href": "lectures/lecture03/lecture03.html#select-to-keep-variables",
    "title": "Data Wrangling",
    "section": "select() to keep variables",
    "text": "select() to keep variables\n\nidaho_crashes %>%\n  filter(County == \"Canyon\", Year == 2017) %>% \n  select(IntersectionRelated, Number_Of_Injuries)\n\n# A tibble: 3,191 × 2\n   IntersectionRelated Number_Of_Injuries\n   <chr>                            <dbl>\n 1 N                                    1\n 2 N                                    1\n 3 N                                    0\n 4 Y                                    0\n 5 N                                    1\n 6 N                                    1\n 7 N                                    1\n 8 Y                                    0\n 9 Y                                    0\n10 N                                    0\n# … with 3,181 more rows"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#select-to-exclude-variables",
    "href": "lectures/lecture03/lecture03.html#select-to-exclude-variables",
    "title": "Data Wrangling",
    "section": "select() to exclude variables",
    "text": "select() to exclude variables\n\nidaho_crashes %>% \n  select(-OBJECTID)\n\n# A tibble: 299,135 × 11\n   Severity     Mile_…¹  Year County Numbe…² Numbe…³ Inter…⁴ Accident_Date_Time \n   <chr>          <dbl> <dbl> <chr>    <dbl>   <dbl> <chr>   <dttm>             \n 1 Property Dm… 270.     2005 Oneida       0       0 N       2005-01-01 00:01:00\n 2 Property Dm…   0.004  2005 Canyon       0       0 N       2005-01-01 00:01:00\n 3 Property Dm…   5.25   2005 Canyon       0       0 Y       2005-01-01 00:03:00\n 4 Property Dm…   4.6    2005 Twin …       0       0 N       2005-01-01 00:05:00\n 5 C Injury Ac…   1.62   2005 Koote…       0       2 Y       2005-01-01 00:27:00\n 6 Property Dm…  20.5    2005 Ada          0       0 Y       2005-01-01 00:29:00\n 7 Property Dm…  NA      2005 Canyon       0       0 Y       2005-01-01 00:35:00\n 8 Property Dm…  NA      2005 Latah        0       0 N       2005-01-01 00:45:00\n 9 Property Dm…  NA      2005 Koote…       0       0 N       2005-01-01 01:00:00\n10 Property Dm…  NA      2005 Bonne…       0       0 N       2005-01-01 01:25:00\n# … with 299,125 more rows, 3 more variables: Hour <int>, Month <ord>,\n#   Day <ord>, and abbreviated variable names ¹​Mile_Point,\n#   ²​Number_Of_Fatalities, ³​Number_Of_Injuries, ⁴​IntersectionRelated"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#select-a-range-of-variables",
    "href": "lectures/lecture03/lecture03.html#select-a-range-of-variables",
    "title": "Data Wrangling",
    "section": "select() a range of variables",
    "text": "select() a range of variables\n\nidaho_crashes %>% \n  select(Severity:Mile_Point)\n\n# A tibble: 299,135 × 2\n   Severity            Mile_Point\n   <chr>                    <dbl>\n 1 Property Dmg Report    270.   \n 2 Property Dmg Report      0.004\n 3 Property Dmg Report      5.25 \n 4 Property Dmg Report      4.6  \n 5 C Injury Accident        1.62 \n 6 Property Dmg Report     20.5  \n 7 Property Dmg Report     NA    \n 8 Property Dmg Report     NA    \n 9 Property Dmg Report     NA    \n10 Property Dmg Report     NA    \n# … with 299,125 more rows"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#mutate-to-add-new-variables",
    "href": "lectures/lecture03/lecture03.html#mutate-to-add-new-variables",
    "title": "Data Wrangling",
    "section": "mutate() to add new variables",
    "text": "mutate() to add new variables"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#mutate-to-add-new-variables-1",
    "href": "lectures/lecture03/lecture03.html#mutate-to-add-new-variables-1",
    "title": "Data Wrangling",
    "section": "mutate() to add new variables",
    "text": "mutate() to add new variables\n\nidaho_crashes %>% \n  mutate(total_inj_fat = Number_Of_Injuries + Number_Of_Fatalities)\n\n# A tibble: 299,135 × 13\n   OBJECTID Severity            Mile_Point  Year County  Numbe…¹ Numbe…² Inter…³\n      <dbl> <chr>                    <dbl> <dbl> <chr>     <dbl>   <dbl> <chr>  \n 1    17633 Property Dmg Report    270.     2005 Oneida        0       0 N      \n 2    20294 Property Dmg Report      0.004  2005 Canyon        0       0 N      \n 3    21886 Property Dmg Report      5.25   2005 Canyon        0       0 Y      \n 4    22038 Property Dmg Report      4.6    2005 Twin F…       0       0 N      \n 5    18025 C Injury Accident        1.62   2005 Kooten…       0       2 Y      \n 6    19661 Property Dmg Report     20.5    2005 Ada           0       0 Y      \n 7    26693 Property Dmg Report     NA      2005 Canyon        0       0 Y      \n 8    18787 Property Dmg Report     NA      2005 Latah         0       0 N      \n 9    19633 Property Dmg Report     NA      2005 Kooten…       0       0 N      \n10    22197 Property Dmg Report     NA      2005 Bonnev…       0       0 N      \n# … with 299,125 more rows, 5 more variables: Accident_Date_Time <dttm>,\n#   Hour <int>, Month <ord>, Day <ord>, total_inj_fat <dbl>, and abbreviated\n#   variable names ¹​Number_Of_Fatalities, ²​Number_Of_Injuries,\n#   ³​IntersectionRelated"
  },
  {
    "objectID": "lectures/lecture03/lecture03.html#save-when-you-mutate",
    "href": "lectures/lecture03/lecture03.html#save-when-you-mutate",
    "title": "Data Wrangling",
    "section": "“Save” when you mutate",
    "text": "“Save” when you mutate\n\nidaho_crashes <- idaho_crashes %>% \n  mutate(total_inj_fat = Number_Of_Injuries + Number_Of_Fatalities)\n\n\nMost often when you define a new variable with mutate you’ll also want to save the resulting data frame, often by writing over the original data frame."
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#what-is-tidy-data",
    "href": "lectures/lecture04/lecture04.html#what-is-tidy-data",
    "title": "Tidy Data",
    "section": "What is tidy data?",
    "text": "What is tidy data?\nClean perfect data?"
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#what-is-tidy-data-1",
    "href": "lectures/lecture04/lecture04.html#what-is-tidy-data-1",
    "title": "Tidy Data",
    "section": "What is “tidy” data?",
    "text": "What is “tidy” data?\n\n“Tidy” data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types."
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#what-is-a-dataset",
    "href": "lectures/lecture04/lecture04.html#what-is-a-dataset",
    "title": "Tidy Data",
    "section": "What is a dataset?",
    "text": "What is a dataset?\n\nA dataset is a collection of values, usually either numbers (if quantitative) or strings AKA text data (if qualitative/categorical). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a city) across attributes."
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#what-is-tidy-data-2",
    "href": "lectures/lecture04/lecture04.html#what-is-tidy-data-2",
    "title": "Tidy Data",
    "section": "What is “tidy” data?",
    "text": "What is “tidy” data?\n\n\n\nEach variable forms a column\nEach observation forms a row\nEach type of observational unit forms a table"
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#untidy-column-headers-are-values",
    "href": "lectures/lecture04/lecture04.html#untidy-column-headers-are-values",
    "title": "Tidy Data",
    "section": "Untidy: Column headers are values",
    "text": "Untidy: Column headers are values"
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#tidy",
    "href": "lectures/lecture04/lecture04.html#tidy",
    "title": "Tidy Data",
    "section": "Tidy",
    "text": "Tidy"
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#case-study-billboard-ranking-data",
    "href": "lectures/lecture04/lecture04.html#case-study-billboard-ranking-data",
    "title": "Tidy Data",
    "section": "Case study: Billboard ranking data",
    "text": "Case study: Billboard ranking data\n\n\n# A tibble: 317 × 79\n   artist track date.ent…¹   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8   wk9\n   <chr>  <chr> <date>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 2 Pac  Baby… 2000-02-26    87    82    72    77    87    94    99    NA    NA\n 2 2Ge+h… The … 2000-09-02    91    87    92    NA    NA    NA    NA    NA    NA\n 3 3 Doo… Kryp… 2000-04-08    81    70    68    67    66    57    54    53    51\n 4 3 Doo… Loser 2000-10-21    76    76    72    69    67    65    55    59    62\n 5 504 B… Wobb… 2000-04-15    57    34    25    17    17    31    36    49    53\n 6 98^0   Give… 2000-08-19    51    39    34    26    26    19     2     2     3\n 7 A*Tee… Danc… 2000-07-08    97    97    96    95   100    NA    NA    NA    NA\n 8 Aaliy… I Do… 2000-01-29    84    62    51    41    38    35    35    38    38\n 9 Aaliy… Try … 2000-03-18    59    53    38    28    21    18    16    14    12\n10 Adams… Open… 2000-08-26    76    76    74    69    68    67    61    58    57\n# … with 307 more rows, 67 more variables: wk10 <dbl>, wk11 <dbl>, wk12 <dbl>,\n#   wk13 <dbl>, wk14 <dbl>, wk15 <dbl>, wk16 <dbl>, wk17 <dbl>, wk18 <dbl>,\n#   wk19 <dbl>, wk20 <dbl>, wk21 <dbl>, wk22 <dbl>, wk23 <dbl>, wk24 <dbl>,\n#   wk25 <dbl>, wk26 <dbl>, wk27 <dbl>, wk28 <dbl>, wk29 <dbl>, wk30 <dbl>,\n#   wk31 <dbl>, wk32 <dbl>, wk33 <dbl>, wk34 <dbl>, wk35 <dbl>, wk36 <dbl>,\n#   wk37 <dbl>, wk38 <dbl>, wk39 <dbl>, wk40 <dbl>, wk41 <dbl>, wk42 <dbl>,\n#   wk43 <dbl>, wk44 <dbl>, wk45 <dbl>, wk46 <dbl>, wk47 <dbl>, wk48 <dbl>, …"
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#how-do-we-tidy-data",
    "href": "lectures/lecture04/lecture04.html#how-do-we-tidy-data",
    "title": "Tidy Data",
    "section": "How do we tidy data?",
    "text": "How do we tidy data?\n\nWe use the pivot_longer function from the tidyr package\nThree arguments:\n\nnames_to,\nvalues_to,\nthe column(s) we do or do not want to tidy\n\n\n\n\nAdditional arguments may be necessary in some cases"
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#names_to",
    "href": "lectures/lecture04/lecture04.html#names_to",
    "title": "Tidy Data",
    "section": "names_to",
    "text": "names_to\n\nthe name of the column/variable in the new “tidy” frame containing the column names of the original data frame we want to tidy"
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#names_to-1",
    "href": "lectures/lecture04/lecture04.html#names_to-1",
    "title": "Tidy Data",
    "section": "names_to",
    "text": "names_to"
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#values_to",
    "href": "lectures/lecture04/lecture04.html#values_to",
    "title": "Tidy Data",
    "section": "values_to",
    "text": "values_to\n\nthe name of the column/variable in the “tidy” frame containing the rows and columns of values in the original data frame we want to tidy"
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#values_to-1",
    "href": "lectures/lecture04/lecture04.html#values_to-1",
    "title": "Tidy Data",
    "section": "values_to",
    "text": "values_to"
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#columns-we-do-or-do-not-want-to-tidy",
    "href": "lectures/lecture04/lecture04.html#columns-we-do-or-do-not-want-to-tidy",
    "title": "Tidy Data",
    "section": "Columns we do, or do not, want to tidy",
    "text": "Columns we do, or do not, want to tidy\n\nwant ( c(var1, var2, var3))\nor do not want (-var1, -var2)"
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#columns-we-do-or-do-not-want-to-tidy-1",
    "href": "lectures/lecture04/lecture04.html#columns-we-do-or-do-not-want-to-tidy-1",
    "title": "Tidy Data",
    "section": "Columns we do, or do not, want to tidy",
    "text": "Columns we do, or do not, want to tidy"
  },
  {
    "objectID": "lectures/lecture04/lecture04.html#tidy-billboard-rankings",
    "href": "lectures/lecture04/lecture04.html#tidy-billboard-rankings",
    "title": "Tidy Data",
    "section": "Tidy Billboard Rankings",
    "text": "Tidy Billboard Rankings\n\nbillboard_tidy <- billboard %>% \n  pivot_longer(\n    names_to = \"week\", \n    values_to = \"rank\", \n    wk1:wk76,\n    values_drop_na = TRUE\n  )\nbillboard_tidy\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered week   rank\n   <chr>   <chr>                   <date>       <chr> <dbl>\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk1      87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk2      82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk3      72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk4      77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk5      87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk6      94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   wk7      99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   wk1      91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   wk2      87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   wk3      92\n# … with 5,297 more rows"
  },
  {
    "objectID": "labs/lab04.html#load-packages",
    "href": "labs/lab04.html#load-packages",
    "title": "SPS 502 | Data Science",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(googlesheets4)"
  },
  {
    "objectID": "labs/lab04.html#import-data",
    "href": "labs/lab04.html#import-data",
    "title": "SPS 502 | Data Science",
    "section": "Import Data",
    "text": "Import Data\nWe’re going to import our data from a google sheet:\n\ngapminder_wide <- read_sheet(\"https://docs.google.com/spreadsheets/d/1E8u2GL0aCmWiU2AnpNirfUPxWSJBN50Tyrf5ilapwSw/edit?usp=sharing\")\n\n\nAuthorize tidyverse to use your google account\nType 1 for yes: \n Copy the authorization code:\n\nPaste it into the console:"
  },
  {
    "objectID": "labs/lab04.html#exercise-1",
    "href": "labs/lab04.html#exercise-1",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 1",
    "text": "Exercise 1\nHow many observations and how many variables are in the dataset? What does each row represent? Is the dataset tidy? Enter the code you need to get this information and answer the questions.\n\nglimpse(gapminder_wide)"
  },
  {
    "objectID": "labs/lab04.html#exercise-2",
    "href": "labs/lab04.html#exercise-2",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 2",
    "text": "Exercise 2\nSpoiler: It ain’t tidy! Tidying data can be really tricky in some cases so we’re going to go through this together.\nLet’s look at the data again, but this time we’re going to use the tbl_vars() function from the dyplyr package to just give us a list of all the variables in the dataset.\n\ngapminder_wide %>% \n  tbl_vars()\n\nSo, we see from the output above that each variable has a year at the end of its name (e.g., pop_1952). If we’re interested in analyzing these data, we know we don’t want to have multiple columns for each variable (e.g., a life expectancy column for each year). We want ONE life expectancy variable (column) with values recorded for each year for each country in the dataset. So, let’s use the pivot_longer() function from the dplyr package to make our data tidy:\n\ngapminder_tidy <- gapminder_wide %>% \n  pivot_longer(names_to = c(\".value\", \"year\"),\n               names_sep = \"_\",\n               names_transform = list(year = as.integer),\n              cols = c(-continent, -country))\n\nThis code looks a little bit different than the pivot_longer code we went over in the lecture. For more information about all of these arguments visit the the pivot longer webpage for the tidyrpackage.\n\ngapminder_tidy %>% \n  tbl_vars()"
  },
  {
    "objectID": "labs/lab04.html#exercise-3",
    "href": "labs/lab04.html#exercise-3",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 3",
    "text": "Exercise 3\nFirst, let’s get a sense of our new tidy dataset using the glimpse command:\n\n\n\nAnswer the following:\n\nHow many observations are there?\nHow many variables?\nWhat is the unit of analysis (what does each row represent)? How does this differ from the unit of analysis in the untidy(wide) data you started with?\n\nNext, let’s calculate some basic descriptive statistics to get a sense of our variables."
  },
  {
    "objectID": "labs/lab04.html#exercise-4",
    "href": "labs/lab04.html#exercise-4",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat is the average population in the tidy dataset?"
  },
  {
    "objectID": "labs/lab04.html#exercise-5",
    "href": "labs/lab04.html#exercise-5",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 5",
    "text": "Exercise 5\nAverage population for so many countries over many years isn’t super useful to us. How can we come up with a more meaningful average population statistic? Let’s break the data down a bit to find something more interesting. Write and run code to calculate the average population for each year in the dataset.\n\n\n\nOkay, slightly more useful. A lot more people in 2007 than in 1952!"
  },
  {
    "objectID": "labs/lab04.html#exercise-6",
    "href": "labs/lab04.html#exercise-6",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 6",
    "text": "Exercise 6\nLet’s create a visualization of population over time. One thing we could try is visualizing population over time for each continent. Think about which kind of graph would be best for communicating this then write and run the code (HINT: You’re going to need to add group = continent to your aes() arguments to make the graph legible:"
  },
  {
    "objectID": "labs/Lab05.html#load-the-packages",
    "href": "labs/Lab05.html#load-the-packages",
    "title": "SPS 502 | Data Science",
    "section": "Load the packages",
    "text": "Load the packages\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\n\nThe Data\nToday we will practice data visualization using data on births from the state of North Carolina. Copy, paste and run the code below to load the data.\n\nnc <- read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vTm2WZwNBoQdZhMgot7urbtu8eG7tzAq-60ZJsQ_nupykCAcW0OXebVpHksPWyR4x8xJTVQ8KAulAFS/pub?gid=202410847&single=true&output=csv\")\n\nThe data set that shows up in your Environment is a large data frame. Each observation or case is a birth of a single child. We also call this our “unit of analysis”."
  },
  {
    "objectID": "labs/Lab05.html#exercise-1",
    "href": "labs/Lab05.html#exercise-1",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 1",
    "text": "Exercise 1\nWhat type of variable is R considering the variable habit to be? What variable type is visits? (answer with text)"
  },
  {
    "objectID": "labs/Lab05.html#scatterplots",
    "href": "labs/Lab05.html#scatterplots",
    "title": "SPS 502 | Data Science",
    "section": "Scatterplots",
    "text": "Scatterplots\nScatterplots allow you to investigate the relationship between two numerical variables. While you may already be familiar with this type of plot, let’s view it through the lens of the Grammar of Graphics. Specifically, we will graphically investigate the relationship between the following two numerical variables in the flights data frame:\n\nweeks: length of a pregnancy on the horizontal “x” axis and\nweight: birth weight of a baby in pounds on the vertical “y” axis\n\n\nggplot(data = nc, aes(x = weeks, y = weight)) + \n  geom_point()\n\n\n\n\nLet’s view this plot through the grammar of graphics. Within the ggplot() function call, we specified:\n\nThe data frame to be nc by setting data = nc\nThe aesthetic mapping by setting aes(x = weeks, y = weight)\nThe variable weeks maps to the x-position aesthetic\nThe variable weight maps to the y-position aesthetic.\n\nWe also add a layer to the ggplot() function call using the + sign. The layer in question specifies the geometric object here as points, by specifying geom_point().\n\nFinally, we can also add axis labels and a title to the plot like so. Again we add a new layer, this time a labs or labels layer.\n\nggplot(data = nc, aes(x = weeks, y = weight)) + \n  geom_point() + \n  labs(x = \"Length of pregnancy (in weeks)\", y = \"Birth weight of baby (lbs)\", \n       title = \"Relationship between pregnancy duration and newborn weight\")\n\n\nExercise 2\nIs there a positive or negative relationship between these variables? (text only to answer)\n\n\nExercise 3\nMake a graph showing weeks again on the x axis and the variable gained on the y axis (the amount of weight a mother gained during pregnancy). Include axis labels with measurement units, and a title. (code only to answer)\n\n\nExercise 4\nStudy the code below, and the resulting graphical output. Note a new argument of color = premie inside the aesthetic mapping. The variable premie indicates whether a birth was early (premie) or went full term. Please answer with text:\n **A.** What did adding the argument `color = premie` accomplish? \n \n **B.** How many **variables** are now displayed on this plot?  \n \n **C.** What appears to (roughly) be the pregnancy length cutoff for classifying a newborn as a \"premie\"\" versus a \"full term\". \n\nggplot(data = nc, aes(x = weeks, y = gained, color = premie))+ \n  geom_point() + \n  labs(x = \"Pregnancy length (wks)\", y = \"Maternal weight gain (lbs)\")\n\n\n\n\n\n\nExercise 5\nMake a new scatterplot that shows a mothers age on the x axis (variable called mage) and birth weight of newborns on the y axis (weight). Color the points on the plot based on the gender of the resulting baby (variable called gender). Does there appear to be any strong relationship between a mother’s age and the weight of her newborn? (code and text to answer)"
  },
  {
    "objectID": "labs/Lab05.html#section",
    "href": "labs/Lab05.html#section",
    "title": "SPS 502 | Data Science",
    "section": "",
    "text": "Is there a positive or negative relationship between these variables? (text only to answer)\nMake a graph showing weeks again on the x axis and the variable gained on the y axis (the amount of weight a mother gained during pregnancy). Include axis labels with measurement units, and a title. (code only to answer)\nStudy the code below, and the resulting graphical output. Note that I added a new argument of color = premie inside the aesthetic mapping. The variable premie indicates whether a birth was early (premie) or went full term. Please answer with text:\nA. What did adding the argument color = premie accomplish?\nB. How many variables are now displayed on this plot?\nC. What appears to (roughly) be the pregnancy length cutoff for classifying a newborn as a “premie”” versus a “full term”.\n\n\nggplot(data = nc, aes(x = weeks, y = gained, color = premie))+ \n  geom_point() + \n  labs(x = \"Pregnancy length (wks)\", y = \"Maternal weight gain (lbs)\")\n\n\n\n\n\nMake a new scatterplot that shows a mothers age on the x axis (variable called mage) and birth weight of newborns on the y axis (weight). Color the points on the plot based on the gender of the resulting baby (variable called gender). Does there appear to be any strong relationship between a mother’s age and the weight of her newborn? (code and text to answer)\n\n\nMake sure your document is knitting, and that your html file includes Exercise headers, text, and code. Note that knitting automatically saves your Rmd file too!"
  },
  {
    "objectID": "labs/Lab05.html#histograms",
    "href": "labs/Lab05.html#histograms",
    "title": "SPS 502 | Data Science",
    "section": "Histograms",
    "text": "Histograms\nHistograms are useful plots for showing how many elements of a single numerical variable fall in specified bins. This is a very useful way to get a sense of the distribution of your data. Histograms are often one of the first steps in exploring data visually.\nFor instance, to look at the distribution of pregnancy duration (variable called weeks), copy, paste and run the following in a new code chunk:\n\nggplot(data = nc, aes(x = weeks))+ \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nA few things to note here:\n\nThere is only one variable being mapped in aes(): the single numerical variable weeks. You don’t need to compute the y-aesthetic: R calculates it automatically.\nWe set the geometric object as geom_histogram()\nThe warning message encourages us to specify the number of bins on the histogram, as R chose 30 for us.\n\nWe can change the binwidth (and thus the number of bins), as well as the colors like so.\n\nggplot(data = nc, aes(x = weeks))+ \n  geom_histogram(binwidth = 1, color = \"white\", fill = \"steelblue\")\n\n\n\n\nNote that none of these arguments went inside the aesthetic mapping argument as they do not specifically represent mappings of variables.\n\nExercise 6\nInspect the histogram of the weeks variable. Answer each of the following with text.\n**A.** The y axis is labeled **count**. What is specifically being counted in this case? Hint: think about what each case is in this data set. \n\n**B.** What appears to be roughly the average length of pregnancies in weeks?\n\n**C.** If we changed the binwidth to 100, how many bins would there be? Roughly how many cases would be in each bin?\n\n\nExercise 7\nMake a histogram of the birth weight of newborns (which is in lbs), including a title and axis labels. (code only to answer)"
  },
  {
    "objectID": "labs/Lab05.html#boxplots",
    "href": "labs/Lab05.html#boxplots",
    "title": "SPS 502 | Data Science",
    "section": "Boxplots",
    "text": "Boxplots\nWhile histograms can help to show the distribution of data, boxplots have much more flexibility, and can provide even more information in a single graph. The y aesthetic is the numeric variable you want to include in the boxplot, and the x aesthetic is a grouping variable. For instance, below we set gender as the aesthetic mapping for x, and gained as the aesthetic mapping for y. This creates a boxplot of the weight gained for mothers that had male and female newborns. Note that the fill argument is not necessary, but sets a color for the boxplots.\n\nggplot(data = nc, aes(x = gender, y = gained)) +\n  geom_boxplot(fill = \"sienna\")\n\n\n\n\n\nFor review, these are the different parts of the boxplot: ’\n\nThe bottom of the “box” portion represents the 25th percentile (1st quartile)\nThe horizontal line in the “box” shows the median (50th percentile, 2nd quartile)\nThe top of the “box” represents the 75th percentile (3rd quartile)\nThe height of each “box”, i.e. the value of the 3rd quartile minus the value of the 1st quartile, is called the interquartile range (IQR). It is a measure of spread of the middle 50% of values. Longer boxes indicating more variability.\nThe “whiskers” extending out from the bottoms and tops of the boxes represent points less than the 25th percentile and greater than the 75th percentiles respectively. They extend out no more than 1.5 x IQR units away from either end of the boxes. The length of these whiskers show how the data outside the middle 50% of values vary. Longer whiskers indicate more variability.\nThe dots represent values falling outside the whiskers or outliers. The definition of an outlier is somewhat arbitrary and not absolute. In this case, they are defined by the length of the whiskers, which are no more than 1.5 x IQR units long.\n\n\n\nExercise 9\nMake a boxplot of the weight gained by moms, split by the maturity status of the mothers (mature). Include axis labels and a title on your plot. Is the median weight gain during pregnancy larger for younger or older moms? (text and code)\n\n\nExercise 10\nMake a boxplot of pregnancy duration in weeks by smoking habit. Is the duration of pregnancy more variable for smokers or non-smokers? (i.e. which group has the greater spread for the variable weeks?). (code and text to answer)"
  },
  {
    "objectID": "hw/hw02.html",
    "href": "hw/hw02.html",
    "title": "SPS 502 | Data Science",
    "section": "",
    "text": "Due by 11:59 p.m., Friday October 7"
  },
  {
    "objectID": "hw/hw02.html#logistics-and-tips-for-success",
    "href": "hw/hw02.html#logistics-and-tips-for-success",
    "title": "SPS 502 | Data Science",
    "section": "Logistics and tips for success",
    "text": "Logistics and tips for success\nThis lab will require you to write some of your own code (similar to lab 2). Read these instructions carefully as you work through the homework assignment.\n\nHow to answer questions\nSome questions will direct you to write code, while others will direct you to write a response. You’ll see TYPE YOUR ANSWER HERE in your RMarkdown file in places where I expect you to write a response. And you’ll see code blocks with #Delete and enter your code where I expect you to write your own code.\n\n\nCopying/pasting and writing code\nCopying and editing code from other answers or examples is a great way to both save time and learn. If you copy code over and something isn’t working, look carefully at the edits you made to see whether you missed something in the copying/pasting process. Also, be sure you’ve edited ALL the variable/object names appropriately so you’re accomplishing what the question is asking you to do. You should also make sure that you’re typing the variable and object names EXACTLY as they appear in the dataset or environment pane (spelling and case must match exactly).\n\n\nIf you’re stuck\nIf you get stuck, don’t panic! Before you ask for help, give yourself a few minutes to figure it out. Search ModernDive, look at your previous labs, and google your problem with “tidyverse”. Also, step away from your computer for a few minutes if you need a break. Finally (and this doesn’t have to be the last step), ask for help on Slack!"
  },
  {
    "objectID": "hw/hw02.html#load-packages",
    "href": "hw/hw02.html#load-packages",
    "title": "SPS 502 | Data Science",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)"
  },
  {
    "objectID": "hw/hw02.html#import-data",
    "href": "hw/hw02.html#import-data",
    "title": "SPS 502 | Data Science",
    "section": "Import Data",
    "text": "Import Data\nNext, let’s load the data:\n\n# Load raw data\nidaho_crashes_raw <- read_csv(\"data/Crash_Data_2005__Present.csv\")\n\nRows: 299135 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Severity, County, IntersectionRelated\ndbl  (5): OBJECTID, Mile_Point, Accident_Year, Number_Of_Fatalities, Number_...\ndttm (1): Accident_Date_Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nJust like last time, we will use the lubridate package to create new variables. The code should look a little less confusing than it did when you first saw it in homework 1.\n\nidaho_crashes_clean <- idaho_crashes_raw %>% \n  mutate(Hour= hour(Accident_Date_Time),\n         Month = month(Accident_Date_Time, label = TRUE, abbr = TRUE),\n         Day = wday(Accident_Date_Time, label = TRUE, abbr = TRUE)) %>%\n    rename(Year = Accident_Year)"
  },
  {
    "objectID": "labs/Lab05.html#faceting",
    "href": "labs/Lab05.html#faceting",
    "title": "SPS 502 | Data Science",
    "section": "Faceting",
    "text": "Faceting\nFaceting is used when we’d like to create small multiples of the same plot over a different categorical variable. By default, all of the small multiples will have the same vertical axis.\nFor example, suppose we were interested in looking at whether pregnancy length varied by the maturity status of a mother (column name mature). This is what is meant by “the distribution of one variable over another variable”: weeks is one variable and mature is the other variable. In order to look at histograms of weeks for older and more mature mothers, we add a plot layer facet_wrap(~ mature, ncol = 1). The ncol = 1 argument just tells R to stack the two histograms into one column.\n\nggplot(data = nc, aes(x = weeks)) +\n  geom_histogram(binwidth = 1, color = \"white\", fill = \"steelblue\") +\n  facet_wrap(~ mature, ncol = 1)\n\n\n\n\n\nExercise 8\nMake a histogram of newborn birth weight split by gender of the child. Set the binwidth to 0.5. Which gender appears to have a slightly larger average birth weight? (code and text to answer)"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#data-idaho-county-demographics-and-2020-voting",
    "href": "lectures/lecture05/lecture05.html#data-idaho-county-demographics-and-2020-voting",
    "title": "Data Visualization II",
    "section": "Data: Idaho County Demographics and 2020 Voting",
    "text": "Data: Idaho County Demographics and 2020 Voting\nData on 2020 Presidential election results, income, age, education, total population, and median home value.\n\nglimpse(idaho_county_merged)\n\nRows: 44\nColumns: 23\n$ county_fips           <dbl> 16001, 16003, 16005, 16007, 16009, 16011, 16013,…\n$ NAME                  <chr> \"Ada County, Idaho\", \"Adams County, Idaho\", \"Ban…\n$ median_incomeE        <dbl> 69952, 50309, 51977, 52829, 47983, 58260, 64627,…\n$ median_incomeM        <dbl> 1549, 3186, 2046, 3650, 2962, 2673, 4953, 7210, …\n$ median_ageE           <dbl> 37.2, 55.3, 34.1, 39.4, 46.0, 34.1, 43.7, 54.4, …\n$ median_ageM           <dbl> 0.2, 1.3, 0.3, 0.9, 0.5, 0.3, 0.6, 0.4, 0.4, 0.3…\n$ total_populationE     <dbl> 469473, 4200, 86742, 6054, 9231, 46246, 22729, 7…\n$ total_populationM     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ median_valueE         <dbl> 298600, 228500, 167300, 154500, 177500, 168200, …\n$ median_valueM         <dbl> 3927, 20118, 3431, 11139, 8795, 7821, 32716, 227…\n$ pct_collegeE          <dbl> 40.1, 18.8, 27.0, 18.3, 17.9, 20.5, 39.3, 28.4, …\n$ pct_collegeM          <dbl> 0.8, 3.0, 1.3, 2.9, 3.1, 2.2, 3.6, 3.4, 2.1, 1.4…\n$ county_name           <chr> \"ADA\", \"ADAMS\", \"BANNOCK\", \"BEAR LAKE\", \"BENEWAH…\n$ totalvotes            <dbl> 259389, 2586, 39553, 3315, 4973, 19996, 13289, 4…\n$ democrat_votes        <dbl> 120539, 591, 14682, 350, 977, 4124, 8919, 1204, …\n$ libertarian_votes     <dbl> 5310, 34, 947, 25, 58, 307, 168, 67, 401, 1288, …\n$ other_votes           <dbl> 2841, 20, 593, 26, 60, 270, 170, 64, 255, 648, 7…\n$ republican_votes      <dbl> 130699, 1941, 23331, 2914, 3878, 15295, 4032, 34…\n$ pct_democrat_votes    <dbl> 46.470359, 22.853828, 37.119814, 10.558069, 19.6…\n$ pct_libertarian_votes <dbl> 2.0471184, 1.3147718, 2.3942558, 0.7541478, 1.16…\n$ pct_other_votes       <dbl> 1.0952662, 0.7733952, 1.4992542, 0.7843137, 1.20…\n$ pct_republican_votes  <dbl> 50.38726, 75.05800, 58.98668, 87.90347, 77.98110…\n$ win_candidate         <chr> \"Trump\", \"Trump\", \"Trump\", \"Trump\", \"Trump\", \"Tr…"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#extra-stuff",
    "href": "lectures/lecture05/lecture05.html#extra-stuff",
    "title": "Data Visualization II",
    "section": "Extra Stuff",
    "text": "Extra Stuff\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\n::: {.column width=“50%”}"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#code",
    "href": "lectures/lecture05/lecture05.html#code",
    "title": "Data Visualization II",
    "section": "Code",
    "text": "Code"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#plot",
    "href": "lectures/lecture05/lecture05.html#plot",
    "title": "Data Visualization II",
    "section": "Plot",
    "text": "Plot"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#income-and-education-visualization",
    "href": "lectures/lecture05/lecture05.html#income-and-education-visualization",
    "title": "Data Visualization II",
    "section": "Income and Education Visualization",
    "text": "Income and Education Visualization\nggplot(data = idaho_county_merged, \n       mapping = aes(x = pct_collegeE, y = median_incomeE,\n                     color = win_candidate)) +\n  geom_point() +\n  labs(title = \"Education and Median Income\",\n       subtitle = \"Linear relationship between education and income\",\n       x = \"Pct Adults w/ Bachelor's Degree or Higher\", y = \"Median Income\",\n       color = \"Winning Pres. Candidate (2020)\",\n       caption = \"Source:Education and income data from U.S. Census Bureau. \\n Voting data from the Harvard Dataverse\") +"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#coding-out-loud",
    "href": "lectures/lecture05/lecture05.html#coding-out-loud",
    "title": "Data Visualization II",
    "section": "Coding Out Loud",
    "text": "Coding Out Loud\nStart with the idaho_county_merged data frame\n\n\n```{r}\n#| output-location: column\n#| code-line-numbers: \"|4\"\nggplot(data = idaho_county_merged)\n```"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#coding-out-loud-1",
    "href": "lectures/lecture05/lecture05.html#coding-out-loud-1",
    "title": "Data Visualization II",
    "section": "Coding Out Loud",
    "text": "Coding Out Loud\nStart with the idaho_county_merged data frame, map college attainment to the x-axis\n\n\n```{r}\n#| output-location: column\n#| code-line-numbers: \"|5\"\nggplot(data = idaho_county_merged,\n       mapping = aes(x = pct_collegeE))\n```"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#coding-out-loud-2",
    "href": "lectures/lecture05/lecture05.html#coding-out-loud-2",
    "title": "Data Visualization II",
    "section": "Coding Out Loud",
    "text": "Coding Out Loud\nStart with the idaho_county_merged data frame, map college attainment to the x-axis and map median income to the y-axis\n\n\n```{r}\n#| output-location: column\n#| code-line-numbers: \"|6\"\nggplot(data = idaho_county_merged,\n       mapping = aes(x = pct_collegeE, \n                     y = median_incomeE))\n```"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#coding-out-loud-3",
    "href": "lectures/lecture05/lecture05.html#coding-out-loud-3",
    "title": "Data Visualization II",
    "section": "Coding Out Loud",
    "text": "Coding Out Loud\nStart with the idaho_county_merged data frame, map college attainment to the x-axis and map median income to the y-axis. Represent each observation with a point\n\n\n```{r}\n#| output-location: column\n#| code-line-numbers: \"|7\"\nggplot(data = idaho_county_merged,\n       mapping = aes(x = pct_collegeE, \n                     y = median_incomeE))+\n  geom_point()\n```"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#coding-out-loud-4",
    "href": "lectures/lecture05/lecture05.html#coding-out-loud-4",
    "title": "Data Visualization II",
    "section": "Coding Out Loud",
    "text": "Coding Out Loud\nStart with the idaho_county_merged data frame, map college attainment to the x-axis and map median income to the y-axis. Represent each observation with a point and map election winner to the color of each point.\n\n\n```{r}\n#| output-location: column\n#| code-line-numbers: \"|8\"\nggplot(data = idaho_county_merged,\n       mapping = aes(x = pct_collegeE, \n                     y = median_incomeE,\n                     color = win_candidate))+\n  geom_point()\n```"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#coding-out-loud-5",
    "href": "lectures/lecture05/lecture05.html#coding-out-loud-5",
    "title": "Data Visualization II",
    "section": "Coding Out Loud",
    "text": "Coding Out Loud\nStart with the idaho_county_merged data frame, map college attainment to the x-axis and map median income to the y-axis. Represent each observation with a point and map election winner to the color of each point. Title the plot “Education and Median Income”.\n\n\n```{r}\n#| output-location: column\n#| code-line-numbers: |8\nggplot(data = idaho_county_merged, \n       mapping = aes(x = pct_collegeE, y = median_incomeE,\n                     color = win_candidate)) +\n  geom_point() +\n  labs(title = \"Education and Median Income\")\n```"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#coding-out-loud-6",
    "href": "lectures/lecture05/lecture05.html#coding-out-loud-6",
    "title": "Data Visualization II",
    "section": "Coding Out Loud",
    "text": "Coding Out Loud\nStart with the idaho_county_merged data frame, map college attainment to the x-axis and map median income to the y-axis. Represent each observation with a point and map election winner to the color of each point. Title the plot “Education and Median Income”. Add the subtitle, “Linear relationship between education and income”\n\n\n```{r}\n#| output-location: column\n#| code-line-numbers: \"|9\"\nggplot(data = idaho_county_merged, \n       mapping = aes(x = pct_collegeE, y = median_incomeE,\n                     color = win_candidate)) +\n  geom_point() +\n  labs(title = \"Education and Median Income\",\n       subtitle = \"Linear relationship between education and income\")\n```"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#coding-out-loud-7",
    "href": "lectures/lecture05/lecture05.html#coding-out-loud-7",
    "title": "Data Visualization II",
    "section": "Coding Out Loud",
    "text": "Coding Out Loud\nStart with the idaho_county_merged data frame, map college attainment to the x-axis and map median income to the y-axis. Represent each observation with a point and map election winner to the color of each point. Title the plot “Education and Median Income”. Add the subtitle, “Linear relationship between education and income”. Label the x and y axes “Pct Adults w/ Bachelor’s Degree or Higher” and “Median Income”, respectively\n\n\n```{r}\n#| output-location: column\n#| code-line-numbers: \"|10|11\"\nggplot(data = idaho_county_merged, \n       mapping = aes(x = pct_collegeE, y = median_incomeE,\n                     color = win_candidate)) +\n  geom_point() +\n  labs(title = \"Education and Median Income\",\n       subtitle = \"Linear relationship between education and income\",\n       x = \"Pct Adults w/ Bachelor's Degree or Higher\", \n       y = \"Median Income\")\n```"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#coding-out-loud-8",
    "href": "lectures/lecture05/lecture05.html#coding-out-loud-8",
    "title": "Data Visualization II",
    "section": "Coding Out Loud",
    "text": "Coding Out Loud\nStart with the idaho_county_merged data frame, map college attainment to the x-axis and map median income to the y-axis. Represent each observation with a point and map election winner to the color of each point. Title the plot “Education and Median Income”. Add the subtitle, “Linear relationship between education and income”. Label the x and y axes “Pct Adults w/ Bachelor’s Degree or Higher” and “Median Income”, respectively, label the legend “Winning Pres. Candidate (2020)”\n\n\n```{r}\n#| output-location: column\n#| code-line-numbers: \"|12\"\nggplot(data = idaho_county_merged, \n       mapping = aes(x = pct_collegeE, y = median_incomeE,\n                     color = win_candidate)) +\n  geom_point() +\n  labs(title = \"Education and Median Income\",\n       subtitle = \"Linear relationship between education and income\",\n       x = \"Pct Adults w/ Bachelor's Degree or Higher\", \n       y = \"Median Income\",\n       color = \"Winning Pres. Candidate (2020)\")\n```"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#coding-out-loud-9",
    "href": "lectures/lecture05/lecture05.html#coding-out-loud-9",
    "title": "Data Visualization II",
    "section": "Coding Out Loud",
    "text": "Coding Out Loud\nStart with the idaho_county_merged data frame, map college attainment to the x-axis and map median income to the y-axis. Represent each observation with a point and map election winner to the color of each point. Title the plot “Education and Median Income”. Add the subtitle, “Linear relationship between education and income”. Label the x and y axes “Pct Adults w/ Bachelor’s Degree or Higher” and “Median Income”, respectively, label the legend “Winning Pres. Candidate (2020)”, and add a caption for the data source.\n\n\n```{r}\n#| output-location: column\n#| code-line-numbers: \"|13\"\nggplot(data = idaho_county_merged, \n       mapping = aes(x = pct_collegeE, y = median_incomeE,\n                     color = win_candidate)) +\n  geom_point() +\n  labs(title = \"Education and Median Income\",\n       subtitle = \"Linear relationship between education and income\",\n       x = \"Pct Adults w/ Bachelor's Degree or Higher\", \n       y = \"Median Income\",\n       color = \"Winning Pres. Candidate (2020)\",\n       caption = \"Source:Education and income data from U.S. Census Bureau. \\n Voting data from the Harvard Dataverse\")\n```"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#argument-names",
    "href": "lectures/lecture05/lecture05.html#argument-names",
    "title": "Data Visualization II",
    "section": "Argument names",
    "text": "Argument names\n\nYou can omit the names of first two arguments when building plots with ggplot().\n\n\n\n\nggplot(data = idaho_county_merged, \n       mapping = aes(x = pct_collegeE, y = median_incomeE,\n                     color = win_candidate)) +\n  geom_point() +\n  labs(title = \"Education and Median Income\",\n       subtitle = \"Linear relationship between education and income\",\n       x = \"Pct Adults w/ Bachelor's Degree or Higher\", \n       y = \"Median Income\",\n       color = \"Winning Pres. Candidate (2020)\",\n       caption = \"Source:Education and income data from U.S. Census Bureau. \\n Voting data from the Harvard Dataverse\") +\n scale_color_manual(values = c(\"blue\",\"red\")) \n\n\n\nggplot(idaho_county_merged, \n       aes(x = pct_collegeE, y = median_incomeE,\n                     color = win_candidate)) +\n  geom_point() +\n  labs(title = \"Education and Median Income\",\n       subtitle = \"Linear relationship between education and income\",\n       x = \"Pct Adults w/ Bachelor's Degree or Higher\", \n       y = \"Median Income\",\n       color = \"Winning Pres. Candidate (2020)\",\n       caption = \"Source:Education and income data from U.S. Census Bureau. \\n Voting data from the Harvard Dataverse\") +\n scale_color_manual(values = c(\"blue\",\"red\"))"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#aesthetics-options",
    "href": "lectures/lecture05/lecture05.html#aesthetics-options",
    "title": "Data Visualization II",
    "section": "Aesthetics options",
    "text": "Aesthetics options\nCommonly used characteristics of plotting characters that can be mapped to a specific variable in the data are\n\ncolor\nshape\nsize\nalpha (transparency)"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#color",
    "href": "lectures/lecture05/lecture05.html#color",
    "title": "Data Visualization II",
    "section": "Color",
    "text": "Color\n\n\nggplot(data = idaho_county_merged, \n       mapping = aes(x = pct_collegeE, y = median_incomeE,\n                     color = win_candidate)) +\n  geom_point() +\n  labs(title = \"Education and Median Income\",\n       subtitle = \"Linear relationship between education and income\",\n       x = \"Pct Adults w/ Bachelor's Degree or Higher\", \n       y = \"Median Income\",\n       color = \"Winning Pres. Candidate (2020)\")"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#shape",
    "href": "lectures/lecture05/lecture05.html#shape",
    "title": "Data Visualization II",
    "section": "Shape",
    "text": "Shape\n\n\nggplot(data = idaho_county_merged, \n       mapping = aes(x = pct_collegeE, y = median_incomeE,\n                     shape = win_candidate)) +\n  geom_point() +\n  labs(title = \"Education and Median Income\",\n       subtitle = \"Linear relationship between education and income\",\n       x = \"Pct Adults w/ Bachelor's Degree or Higher\", \n       y = \"Median Income\",\n       shape = \"Winning Pres. Candidate (2020)\")"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#alpha",
    "href": "lectures/lecture05/lecture05.html#alpha",
    "title": "Data Visualization II",
    "section": "Alpha",
    "text": "Alpha\n\n\nggplot(data = idaho_county_merged, \n       mapping = aes(x = pct_collegeE, y = median_incomeE,\n                     color = win_candidate,\n                     size = total_populationE,\n                     alpha = pct_democrat_votes)) +\n  geom_point() +\n  labs(title = \"Education and Median Income\",\n       subtitle = \"Linear relationship between education and income\",\n       x = \"Pct Adults w/ Bachelor's Degree or Higher\", \n       y = \"Median Income\",\n       color = \"Winning Pres. Candidate (2020)\")"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#size",
    "href": "lectures/lecture05/lecture05.html#size",
    "title": "Data Visualization II",
    "section": "Size",
    "text": "Size\n\n\nggplot(data = idaho_county_merged, \n       mapping = aes(x = pct_collegeE, y = median_incomeE,\n                     color = win_candidate,\n                     size = total_populationM)) +\n  geom_point() +\n  labs(title = \"Education and Median Income\",\n       subtitle = \"Linear relationship between education and income\",\n       x = \"Pct Adults w/ Bachelor's Degree or Higher\", \n       y = \"Median Income\",\n       color = \"Winning Pres. Candidate (2020)\")"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#extra-stuff-pulling-census-data-with-tidycensus",
    "href": "lectures/lecture05/lecture05.html#extra-stuff-pulling-census-data-with-tidycensus",
    "title": "Data Visualization II",
    "section": "Extra Stuff: Pulling Census Data with tidycensus",
    "text": "Extra Stuff: Pulling Census Data with tidycensus\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#extra-stuff-visualizing-census-data-with-tidycensus",
    "href": "lectures/lecture05/lecture05.html#extra-stuff-visualizing-census-data-with-tidycensus",
    "title": "Data Visualization II",
    "section": "Extra Stuff: Visualizing Census Data with tidycensus",
    "text": "Extra Stuff: Visualizing Census Data with tidycensus"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#grammar-of-graphics",
    "href": "lectures/lecture05/lecture05.html#grammar-of-graphics",
    "title": "Data Visualization II",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\n\nA grammar of graphics is a tool that enables us to concisely describe the components of a graphic\nA statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects. # ggplot Review"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#anatomy-of-a-ggplot",
    "href": "lectures/lecture05/lecture05.html#anatomy-of-a-ggplot",
    "title": "Data Visualization II",
    "section": "Anatomy of a ggplot",
    "text": "Anatomy of a ggplot\n\nggplot() is the main function in ggplot2\nPlots are constructed in layers\nStructure of the code for plots can be summarized as\n\n\n\n\n\nFor help with ggplot2, see ggplot2.tidyverse.org"
  },
  {
    "objectID": "lectures/lecture05/lecture05.html#coding-out-loud-10",
    "href": "lectures/lecture05/lecture05.html#coding-out-loud-10",
    "title": "Data Visualization II",
    "section": "Coding Out Loud",
    "text": "Coding Out Loud\nStart with the idaho_county_merged data frame, map college attainment to the x-axis and map median income to the y-axis. Represent each observation with a point and map election winner to the color of each point. Title the plot “Education and Median Income”. Add the subtitle, “Linear relationship between education and income”. Label the x and y axes “Pct Adults w/ Bachelor’s Degree or Higher” and “Median Income”, respectively, label the legend “Winning Pres. Candidate (2020)”, and add a caption for the data source. Oh! AND fix the candidate name colors with one more layer of code for ggplot\n\n\n```{r}\n#| output-location: column\n#| code-line-numbers: \"|14\"\nggplot(data = idaho_county_merged, \n       mapping = aes(x = pct_collegeE, y = median_incomeE,\n                     color = win_candidate)) +\n  geom_point() +\n  labs(title = \"Education and Median Income\",\n       subtitle = \"Linear relationship between education and income\",\n       x = \"Pct Adults w/ Bachelor's Degree or Higher\", \n       y = \"Median Income\",\n       color = \"Winning Pres. Candidate (2020)\",\n       caption = \"Source:Education and income data from U.S. Census Bureau. \\n Voting data from the Harvard Dataverse\") +\n scale_color_manual(values = c(\"blue\",\"red\")) \n```"
  },
  {
    "objectID": "finalproject/finalproject.html",
    "href": "finalproject/finalproject.html",
    "title": "SPS 502 | Data Science",
    "section": "",
    "text": "For your final project, you will take a dataset from the wild, explore it, wrangle and clean it, tell a story with it, and make inferences about it using regression analysis and other statistical tools."
  },
  {
    "objectID": "finalproject/finalproject.html#your-own-data",
    "href": "finalproject/finalproject.html#your-own-data",
    "title": "SPS 502 | Data Science",
    "section": "Your own data",
    "text": "Your own data\nUse a dataset from your place of employment, previous data you have worked with, or data you have collected yourself."
  },
  {
    "objectID": "finalproject/finalproject.html#data-from-the-internet",
    "href": "finalproject/finalproject.html#data-from-the-internet",
    "title": "SPS 502 | Data Science",
    "section": "Data from the internet",
    "text": "Data from the internet\nGo to Google Dataset Search (or anywhere else online), find an interesting dataset and ask questions about it. Here are some different high-quality datasets:\n\nNonprofit management\n\nU.S. Charities and Non-profits: All of the charities and nonprofits registered with the IRS1\nNonprofit Grants 2010 to 2016: Nonprofit grants made in the US as listed in Schedule I of the IRS 990 tax form between 2010 to 20162\n\n\n\nFederal, state, and local government management\n\nTransparent Idaho: Financial and personnel data for Idaho state government\nDeadly traffic accidents in the UK (2015): List of all traffic-related deaths in the UK in 20153\nFirefighter Fatalities in the United States: Name, rank, and cause of death for all firefighters killed since 20004\nFederal Emergencies and Disasters, 1953–Present: Every federal emergency or disaster declared by the President of the United States since 19535\nGlobal Terrorism Database (1970–2016): 170,000 terrorist attacks worldwide, 1970-20166\nCity of Austin 311 Unified Data: All 311 calls to the City of Austin since 20147\n\n\n\nHigher education\n\nIntegreated Postsecondary Education Data System (IPEDS): IPEDS gathers information from every college, university, and technical and vocational institution that participates in the federal student financial aid programs. The Higher Education Act of 1965, as amended, requires that institutions that participate in federal student aid programs report data on enrollments, program completions, graduation rates, faculty and staff, finances, institutional prices, and student financial aid.\nNational Longitudinal Survey of Freshmen (NLSF): The National Longitudinal Survey of Freshmen (NLSF) followed a cohort of first-time freshman at selective colleges and universities through their college careers. Equal numbers of whites, blacks, Hispanics, and Asians were sampled at each of the 28 participating schools. Among other uses, the data has been collected with the testing of several competing theories of minority under performance in college in mind.\n\n\n\nPolisci\n\nComparative Political Dataset: The “Comparative Political Data Set” (CPDS) is a collection of political and institutional country-level data provided by Prof. Dr. Klaus Armingeon and collaborators at the University of Berne. It consists of annual data for 36 democratic countries for the period of 1960 to 2017 or since their transition to democracy.\nCorrelates of State Policy: The Correlates of State Policy Project aims to compile, disseminate, and encourage the use of data relevant to U.S. state policy research, tracking policy differences across and change over time in the 50 states.\nfivethirtyeight: Datasets and code published by the data journalism website ‘FiveThirtyEight’ available at https://github.com/fivethirtyeight/data."
  },
  {
    "objectID": "finalproject/finalproject.html#requirements",
    "href": "finalproject/finalproject.html#requirements",
    "title": "SPS 502 | Data Science",
    "section": "Requirements",
    "text": "Requirements\nIn your final paper, you need to include at least one of each of the following elements (i.e. at least one plot, but more is fine; at least one regression model, but more is fine):\n\nA plot of a single variable (like a histogram; see ModernDive 2)\nA plot of multiple variables (like a scatterplot; see ModernDive 2)\n2-3 hypotheses that you will test\nA multiple regression model (see ModernDive 5, 6, and 10)"
  },
  {
    "objectID": "finalproject/finalproject.html#outline",
    "href": "finalproject/finalproject.html#outline",
    "title": "SPS 502 | Data Science",
    "section": "Outline",
    "text": "Outline\nHere is a suggested outline for your final report:\n\nExecutive summary: one-page summary of your questions, methods, findings, and recommendations\nIntroduction and description of research questions: describe the motivation for this study, outline and define what questions you are exploring and why\nData and methods: explain how the data was collected, provide basic summary statistics (tables and figures) of the main variables you’re interested in, and describe what statistical tools you will use to answer your questions (i.e. regression, bootstrapped comparisons of means, etc.)\nResults: answer each of your questions using statistical tools and interpret the results of the different statistical tests you use\nLimitations of the study: provide caveats for your analysis and explain how confident you are in your results\nRecommendations and conclusion: discuss the implications of these findings and make recommendations based on the results\nAppendices: if you want to include tables of summary statistics or tables showing alternative models, you can include them in an appendix instead of in the body of the report itself."
  },
  {
    "objectID": "labs/lab04-key.html#load-packages",
    "href": "labs/lab04-key.html#load-packages",
    "title": "SPS 502 | Data Science",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse)\nlibrary(googlesheets4)"
  },
  {
    "objectID": "labs/lab04-key.html#import-data",
    "href": "labs/lab04-key.html#import-data",
    "title": "SPS 502 | Data Science",
    "section": "Import Data",
    "text": "Import Data\nWe’re going to import our data from a google sheet:\n\ngapminder_wide <- read_sheet(\"https://docs.google.com/spreadsheets/d/1E8u2GL0aCmWiU2AnpNirfUPxWSJBN50Tyrf5ilapwSw/edit?usp=sharing\")\n\n\nAuthorize tidyverse to use your google account\nType 1 for yes: \n Copy the authorization code:\n\nPaste it into the console:"
  },
  {
    "objectID": "labs/lab04-key.html#exercise-1",
    "href": "labs/lab04-key.html#exercise-1",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 1",
    "text": "Exercise 1\nHow many observations and how many variables are in the dataset? What does each row represent? Is the dataset tidy? Enter the code you need to get this information and answer the questions.\n\nglimpse(gapminder_wide)"
  },
  {
    "objectID": "labs/lab04-key.html#exercise-2",
    "href": "labs/lab04-key.html#exercise-2",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 2",
    "text": "Exercise 2\nSpoiler: It ain’t tidy! Tidying data can be really tricky in some cases so we’re going to go through this together.\nLet’s look at the data again, but this time we’re going to use the tbl_vars() function from the dyplyr package to just give us a list of all the variables in the dataset.\n\ngapminder_wide %>% \n  tbl_vars()\n\nSo, we see from the output above that each variable has a year at the end of its name (e.g., pop_1952). If we’re interested in analyzing these data, we know we don’t want to have multiple columns for each variable (e.g., a life expectancy column for each year). We want ONE life expectancy variable (column) with values recorded for each year for each country in the dataset. So, let’s use the pivot_longer() function from the dplyr package to make our data tidy:\n\ngapminder_tidy <- gapminder_wide %>% \n  pivot_longer(names_to = c(\".value\", \"year\"),\n               names_sep = \"_\",\n               names_transform = list(year = as.integer),\n              cols = c(-continent, -country))\n\nThis code looks a little bit different than the pivot_longer code we went over in the lecture. For more information about all of these arguments visit the the pivot longer webpage for the tidyrpackage.\n\ngapminder_tidy %>% \n  tbl_vars()"
  },
  {
    "objectID": "labs/lab04-key.html#exercise-3",
    "href": "labs/lab04-key.html#exercise-3",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 3",
    "text": "Exercise 3\nFirst, let’s get a sense of our new tidy dataset using the glimpse command:\n\nglimpse(gapminder_tidy)\n\nAnswer the following:\n\nHow many observations are there? 1,704\nHow many variables? 6\nWhat is the unit of analysis (what does each row represent)? How does this differ from the unit of analysis in the untidy(wide) data you started with?\n\nCountry-year. The unit of analysis in the untidy dataset was country.\nNext, let’s calculate some basic descriptive statistics to get a sense of our variables."
  },
  {
    "objectID": "labs/lab04-key.html#exercise-4",
    "href": "labs/lab04-key.html#exercise-4",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat is the average population in the tidy dataset?\n\ngapminder_tidy %>% \nsummarize(avg_pop = mean(pop, na.rm=TRUE))"
  },
  {
    "objectID": "labs/lab04-key.html#exercise-5",
    "href": "labs/lab04-key.html#exercise-5",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 5",
    "text": "Exercise 5\nAverage population for so many countries over many years isn’t super useful to us. How can we come up with a more meaningful average population statistic? Let’s break the data down a bit to find something more interesting. Write and run code to calculate the average population for each year in the dataset.\n\ngapminder_tidy %>% \n  group_by(year) %>% \nsummarize(avg_pop_year = mean(pop, na.rm=TRUE))\n\nOkay, slightly more useful. A lot more people in 2007 than in 1952!"
  },
  {
    "objectID": "labs/lab04-key.html#exercise-6",
    "href": "labs/lab04-key.html#exercise-6",
    "title": "SPS 502 | Data Science",
    "section": "Exercise 6",
    "text": "Exercise 6\nLet’s create a visualization of population over time. One thing we could try is visualizing population over time for each continent. Think about which kind of graph would be best for communicating this then write and run the code (HINT: You’re going to need to add group = continent to your aes() arguments to make the graph legible:\n\ngapminder_tidy %>% \n  group_by(year, continent) %>% \n  summarize(avg_pop = mean(pop)) %>% \nggplot(aes(x=year, y=avg_pop, color=continent)) +\n  geom_line()"
  },
  {
    "objectID": "tutorials/tidycensus.html",
    "href": "tutorials/tidycensus.html",
    "title": "SPS 502 | Data Science",
    "section": "",
    "text": "This is adapted from the “Basic usage of tidycensus” page, which can be found here."
  },
  {
    "objectID": "tutorials/tidycensus.html#step-1-install",
    "href": "tutorials/tidycensus.html#step-1-install",
    "title": "SPS 502 | Data Science",
    "section": "Step 1: Install",
    "text": "Step 1: Install\nRun the following code to install and load the necessary packages:\n\ninstall.packages(\"tidycensus\")\nlibrary(tidycensus)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "tutorials/tidycensus.html#step-2-load-and-configure",
    "href": "tutorials/tidycensus.html#step-2-load-and-configure",
    "title": "SPS 502 | Data Science",
    "section": "Step 2: Load and configure",
    "text": "Step 2: Load and configure\n\nNext, you’ll need to get an API key from the U.S. Census Bureau. Follow this link, enter your information and you should get an email with your API key shortly.\nNext, add your api key and run the following code:\n\n\ncensus_api_key(\"YOUR API KEY GOES HERE\", install = TRUE)"
  },
  {
    "objectID": "tutorials/tidycensus.html#step-3-find-variables",
    "href": "tutorials/tidycensus.html#step-3-find-variables",
    "title": "SPS 502 | Data Science",
    "section": "Step 3: Find variables",
    "text": "Step 3: Find variables\nNext you’ll want to get the variables you’d like to use in your project. There are a TON of variables you can get through tidycensus. So the best way to start is to use the load_variables() function to get a searchable table of all the variables you can use. In this case, we’ll call up the 2020 American Community Survey and store the results in an object called tidycensusvariables. We can then browse and search with the View() function (see video for demonstration of this part).\n\ntidycensusvariables2020 <- load_variables(year = 2020, dataset = \"acs5/profile\") \n\n#Run View(tidycensusvariables) in your console\n\nTo make searching easier, we’re going to use the separate() function from tidyr.\n\ntidycensusvariables2020_sep <- tidycensusvariables2020 %>% \n  separate(label, into=c(\"measure\", \"topic\", \"group\", \"variable\", \"variable2\" ), sep=\"!!\")\n\nWarning: Expected 5 pieces. Additional pieces discarded in 100 rows [17, 18, 25,\n26, 165, 166, 167, 168, 169, 170, 171, 172, 181, 182, 183, 184, 325, 326, 333,\n334, ...].\n\n\nWarning: Expected 5 pieces. Missing pieces filled with `NA` in 1064 rows [1, 2,\n3, 4, 7, 8, 11, 12, 19, 20, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, ...].\n\n\nThis code is telling R to take the label variable in tidycensusvariables2020 and separate it into columns based on the seperator “!!”. This is going to make it a lot easier to search using “filter” in View(). Plese see the video for more details about searching for variables."
  },
  {
    "objectID": "tutorials/tidycensus.html#median-income",
    "href": "tutorials/tidycensus.html#median-income",
    "title": "SPS 502 | Data Science",
    "section": "Median Income",
    "text": "Median Income\nTo make a map visualization of median_income in Idaho, you’ll use ggplot() and the fill aesthetic with geom_sf():\n\nggplot(data = id_acs, aes(fill = median_income)) + \n  geom_sf() + \n  scale_fill_distiller(direction = 1) + \n  labs(title = \"  Median Income by County\",\n       caption = \"Data source: 2020 5-year ACS, US Census Bureau\",\n       fill = \"ACS estimate\") + \n  theme_void()\n\n\n\n\nYou can replicate similar maps with any of the variables we pulled into id_acs."
  },
  {
    "objectID": "tutorials/tidycensus.html#step-4-download-data",
    "href": "tutorials/tidycensus.html#step-4-download-data",
    "title": "SPS 502 | Data Science",
    "section": "Step 4: Download Data",
    "text": "Step 4: Download Data\nNow that I’ve searched through the dataframe and found the variables I want to use, I’ll use get_acs() to pull the actual data into a new dataframe.\n\n    id_acs <- get_acs(\n  geography = \"county\",\n  state = \"Idaho\",\n  variables = c(edu_attainment_pct = \"DP02_0068P\",\n                median_income = \"DP03_0062\",\n                snap_pct = \"DP03_0074P\",\n                unaffordable_rent_pct = \"DP04_0142P\",\n                median_age = \"DP05_0018\"),\n  output = \"wide\",\n  geometry = TRUE,\n  year = 2020\n)\n\nGetting data from the 2016-2020 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\nUsing the ACS Data Profile\n\n\nSince tidycensus pulls both the estimated value and the margin of error for each variable, we’ll just keep the estimated value using the code below. Dealing with the margin of error variables is beyond the scope of the course, but you can read more about it here: https://walker-data.com/tidycensus/articles/margins-of-error.html. The codeblock below also removes the “E” at the end of each variable name.\n\nid_acs <- id_acs %>% \n  select(ends_with(\"E\")) %>% \n  rename_with(~str_remove(., 'E'))\n\nSo. With that, you’ve got a nice, tidy dataframe with demographic data ready to analyze."
  }
]